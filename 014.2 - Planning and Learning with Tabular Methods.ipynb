{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd50319-969d-4e29-bc77-36dfcd88fc3d",
   "metadata": {},
   "source": [
    "# Day 14 - Planning and Learning with Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e56b13-694a-4eb8-be59-fbfb752103d3",
   "metadata": {},
   "source": [
    "## Dyna: Integrated Planning, Acting, and Learning\n",
    "\n",
    "* When planning is done online, new information may change the model, influencing the planning process\n",
    "* It may be useful to customize the planning process to consider the most relevant states\n",
    "* If all of the interacting processes are compute-intensive, then resources will have to be divided well between these\n",
    "* Dyna-Q is a simple architecture that represents each step in the learning and planning process in an almost trivial form\n",
    "* Experience plays two roles:\n",
    "    1. Improving the model of the environment, called $model\\ learning$, sometimes $indirect\\ reinforcement\\ learning$\n",
    "    2. Improving the value estimates and/or policy, called $direct\\ reinforcement\\ learning$\n",
    "* Indirect learning is very sample-efficient\n",
    "* Direct methods are much simpler\n",
    "* There is a debate about whether trial-and-error learning or deliberate planning play a larger role in humans\n",
    "* It is important to also realize the similarities between these two processes\n",
    "* Tabular Dyna-Q interleaves them by performing one step of direct RL, followed by $n$ steps of planning\n",
    "* The model for this is learned by simply recording transitions, $S_t,A_t\\rightarrow R_{t+1},S_{t+1}$\n",
    "* Planning is done by randomly sampling from a previously experienced transition, and then applying Q-learning to it\n",
    "* Conceptually, acting and planning can happen simultaneously, but a serial implementation is simpler and ensures correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365c43d-d188-43f3-8f1b-f2e11c37e73f",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{8.1}$\n",
    "\n",
    "#### The nonplanning method looks particularly poor in Figure 8.3 because it is a one-step method; a method using multi-step bootstrapping would do better. Do you think one of the multi-step bootstrapping methods from Chapter 7 could do as well as the Dyna method? Explain why or why not.\n",
    "\n",
    "While $n$-step methods do propagate information further out from the goal after one episode than one-step methods do, they can only do that for the last $n$ states visited in the first episode, and only once. Planning methods will continue propagating the values outward from the goal, every time they happen to sample a state near it.\n",
    "\n",
    "Thus, $n$-step methods are likely less powerful here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9685d2c-f505-4093-ab04-318e680655fb",
   "metadata": {},
   "source": [
    "## When the Model Is Wrong\n",
    "\n",
    "* Models may be incorrect for various reasons\n",
    "* If the model is too optimistic, the policy will try to exploit the rewards predicted by the model, revealing and fixing the model error\n",
    "* If the environment suddenly becomes $better$ than the model predicts, however, the policy might never discover this\n",
    "* One option do address this is to encourage exploration, by trying actions that improve the model\n",
    "* Adding a bonus to the reward, during planning, based on how long ago the action was last taken in real interaction, leads to optimistic estimates\n",
    "* This can be done, for example, by adding $\\kappa\\sqrt{\\tau}$ to the reward, for some small $\\kappa$, where $\\tau$ is the number of time steps that this action has not been tried"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
