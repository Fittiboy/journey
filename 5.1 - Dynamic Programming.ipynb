{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c753f6b8-7c1b-4517-85a4-60d2ee53f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from policy_evaluation import *\n",
    "from policy_improvement import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26806caa-c3a0-43c2-937d-bedaf9cb05b8",
   "metadata": {},
   "source": [
    "# Day 5 - Dynamic Programming\n",
    "\n",
    "Dynamic programming algorithms use a model of the environment to compute optimal policies. This is usually not possible in practical RL problems, but provides a solid theoretical foundation which more realistic methods approximate. DP algorithms are obtained by turning Bellman equations into update rules for value estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b1549-ef65-4779-a6e1-20497e793574",
   "metadata": {},
   "source": [
    "## Policy Evaluation (Prediction)\n",
    "\n",
    "* $Policy\\ evaluation$ approximates the value function $v_\\pi(s)$ of all states under a given policy $\\pi$\n",
    "* It achieves this through iterative updates of all the states values, using the Bellman equations as update rules:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{k+1}(s)&\\doteq\\mathbb E_\\pi\\left[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s\\right] \\\\\n",
    "&=\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_k(s')\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "* $\\{v_k\\}$ can be shown to converge to $v_\\pi$ under this $iterative\\ policy\\ evaluation$\n",
    "* In practice, updating the state values in place, allowing updates of other states to use newer values, speeds up convergence\n",
    "* As it only converges in the limit, a real implementation has to be halted, for example once $\\underset{s\\in\\mathcal{S}}{\\operatorname\\max}|v_{k+1}(s)-v_k(s)|$ is smaller than some algorithm parameter $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceba8c7-8a7b-46c5-bb52-fa3914a00c70",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.1}$\n",
    "\n",
    "#### In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_\\pi(11,down)$? What is $q_\\pi(7, down)$?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(11, down)&=-1+v(terminal)&&=-1 \\\\\n",
    "q_\\pi(7, down)&=-1+v(11)=-1+(-14)&&=-15\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f75805-c25b-4738-baaf-8f4059f2ab52",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.2}$\n",
    "\n",
    "#### In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, $\\mathtt{left}$, $\\mathtt{up}$, $\\mathtt{right}$, and $\\mathtt{down}$, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_\\pi(15)$ for the equiprobable random policy?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(15)&=-1+\\frac{v_\\pi(12)+v_\\pi(13)+v_\\pi(14)+v_\\pi(15)}{4} \\\\\n",
    "\\frac{3}{4}v_\\pi(15)&=\\frac{-4+v_\\pi(12)+v_\\pi(13)+v_\\pi(14)}{4} \\\\\n",
    "v_\\pi(15)&=\\frac{-4+v_\\pi(12)+v_\\pi(13)+v_\\pi(14)}{3} \\\\\n",
    "&=\\frac{-4-22-20-14}{3} \\\\\n",
    "&=-20\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_\\pi(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "As $v_\\pi(15)=v_\\pi(13)$ before the change of the dynamics, $v_\\pi(13)$ remains unchanged even after the dynamics change, since the value of the $\\mathtt{down}$ action remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b0996-8af8-4ed9-94a6-591d35a04a44",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.3}$\n",
    "\n",
    "#### What are the equations analogous to (4.3), (4.4), and (4.5), but for action-value functions instead of state-value functions?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s,a)&\\doteq\\mathbb E_\\pi\\left[G_t|S_t=s,A_t=a\\right] \\\\\n",
    "&=\\mathbb E_\\pi\\left[R_{t+1}+\\gamma q_\\pi(S_{t+1}|A_{t+1})|S_t=s,A_t=a\\right] \\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}q_\\pi(s',a')\\right] \\\\\n",
    "q_{k+1}(s,a)&\\doteq\\mathbb E_\\pi\\left[R_{t+1}+\\gamma q_k(S_{t+1}|A_{t+1})|S_t=s,A_t=a\\right] \\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}q_k(s',a')\\right] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e46049-36e4-4912-91a1-c692f2c8f02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = 15\n",
    "num_actions = 4\n",
    "transitions = np.zeros((num_states, num_states, num_actions))\n",
    "for s in range(num_states):\n",
    "    up, right, down, left = s-4, s+1, s+4, s-1\n",
    "    if s == 0:\n",
    "        up, right, down, left = 0, 0, 0, 0\n",
    "    if s in [1, 2, 3]:\n",
    "        up = s\n",
    "    if s in [3, 7, 11]:\n",
    "        right = s\n",
    "    if s in [12, 13, 14]:\n",
    "        down = s\n",
    "    if s in [4, 8, 12]:\n",
    "        left = s\n",
    "    if s == 14:\n",
    "        right = 0\n",
    "    if s == 11:\n",
    "        down = 0\n",
    "    transitions[s,up,0] = 1\n",
    "    transitions[s,right,1] = 1\n",
    "    transitions[s,down,2] = 1\n",
    "    transitions[s,left,3] = 1\n",
    "rewards = np.ones((num_states, num_states, num_actions)) * -1.0\n",
    "rewards[0,:,:] = 0.0\n",
    "discount = 1.0\n",
    "policy = np.ones((num_states, num_actions)) / 4.0 # Order: up, right, down, left\n",
    "\n",
    "evaluator = PolicyEvaluation(transitions, rewards, discount, policy)\n",
    "\n",
    "evaluator.evaluate(min_delta=1/(10**100))\n",
    "\n",
    "values = np.zeros((num_states+1,1))\n",
    "values[:num_states,:] = evaluator.values\n",
    "values[num_states,:] = evaluator.values[0]\n",
    "values.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d97b5c0-df87-4601-8f9f-d89c34f3bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.],\n",
       "       [  0., -20.,   0.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = 16\n",
    "num_actions = 4\n",
    "transitions = np.zeros((num_states, num_states, num_actions))\n",
    "for s in range(num_states):\n",
    "    up, right, down, left = s-4, s+1, s+4, s-1\n",
    "    if s == 0:\n",
    "        up, right, down, left = 0, 0, 0, 0\n",
    "    if s in [1, 2, 3]:\n",
    "        up = s\n",
    "    if s in [3, 7, 11]:\n",
    "        right = s\n",
    "    if s in [12, 13, 14]:\n",
    "        down = s\n",
    "    if s in [4, 8, 12]:\n",
    "        left = s\n",
    "    if s == 14:\n",
    "        right = 0\n",
    "    if s == 11:\n",
    "        down = 0\n",
    "    if s == 13:\n",
    "        down = 15\n",
    "    if s == 15:\n",
    "        up = 13\n",
    "        right = 14\n",
    "        down = 15\n",
    "        left = 12\n",
    "    transitions[s,up,0] = 1\n",
    "    transitions[s,right,1] = 1\n",
    "    transitions[s,down,2] = 1\n",
    "    transitions[s,left,3] = 1\n",
    "rewards = np.ones((num_states, num_states, num_actions)) * -1.0\n",
    "rewards[0,:,:] = 0.0\n",
    "discount = 1.0\n",
    "policy = np.ones((num_states, num_actions)) / 4.0 # Order: up, right, down, left\n",
    "\n",
    "evaluator = PolicyEvaluation(transitions, rewards, discount, policy)\n",
    "\n",
    "evaluator.evaluate(min_delta=1/(10**100))\n",
    "\n",
    "values = np.zeros((num_states+4,1))\n",
    "values[:num_states,:] = evaluator.values.copy()\n",
    "values[num_states-1,:] = evaluator.values[0]\n",
    "values[17] = evaluator.values[15]\n",
    "values.reshape(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d675cd-9ffc-424b-b8fb-85f06670c4b9",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6aa1ec1-c9ea-4601-b6a6-1d910ef926be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.],\n",
       "       [ 0., -2.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states = 16\n",
    "num_actions = 4\n",
    "transitions = np.zeros((num_states, num_states, num_actions))\n",
    "for s in range(num_states):\n",
    "    up, right, down, left = s-4, s+1, s+4, s-1\n",
    "    if s == 0:\n",
    "        up, right, down, left = 0, 0, 0, 0\n",
    "    if s in [1, 2, 3]:\n",
    "        up = s\n",
    "    if s in [3, 7, 11]:\n",
    "        right = s\n",
    "    if s in [12, 13, 14]:\n",
    "        down = s\n",
    "    if s in [4, 8, 12]:\n",
    "        left = s\n",
    "    if s == 14:\n",
    "        right = 0\n",
    "    if s == 11:\n",
    "        down = 0\n",
    "    if s == 13:\n",
    "        down = 15\n",
    "    if s == 15:\n",
    "        up = 13\n",
    "        right = 14\n",
    "        down = 15\n",
    "        left = 12\n",
    "    transitions[s,up,0] = 1\n",
    "    transitions[s,right,1] = 1\n",
    "    transitions[s,down,2] = 1\n",
    "    transitions[s,left,3] = 1\n",
    "rewards = np.ones((num_states, num_states, num_actions)) * -1.0\n",
    "rewards[0,:,:] = 0.0\n",
    "discount = 1.0\n",
    "policy = np.ones((num_states, num_actions)) / 4.0 # Order: up, right, down, left\n",
    "\n",
    "evaluator = PolicyImprovement(transitions, rewards, discount, policy)\n",
    "\n",
    "evaluator.evaluate(min_delta=1/(10**100))\n",
    "evaluator.improve_policy()\n",
    "evaluator.evaluate(min_delta=1/(10**100))\n",
    "\n",
    "values = np.zeros((num_states+4,1))\n",
    "values[:num_states,:] = evaluator.values.copy()\n",
    "values[num_states-1,:] = evaluator.values[0]\n",
    "values[17] = evaluator.values[15]\n",
    "values.reshape(5,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
