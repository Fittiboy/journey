{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baac4268-e1a4-48ac-8fc8-1dfb7c6f2a27",
   "metadata": {},
   "source": [
    "# Day 11 - $n$-step Bootstrapping\n",
    "\n",
    "MC methods' updates are based on however many steps an episode took. The TD methods we know so far base their updates on a single time step. $n$-step Bootstrapping methods generalize this to allow a smooth transition between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6f4fe-cf43-4a70-a456-d0a65d600891",
   "metadata": {},
   "source": [
    "## $n$-step TD Prediction\n",
    "\n",
    "* Instead of performing an update by bootstrapping from the value estimate of the next state, we can instead bootstrap from the value estimate $n$ steps ahead, using the intermediate rewards for the update\n",
    "* The update target is thus the $n$-step return $G_{t:t+n}$:\n",
    "\n",
    "$$\n",
    "G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nV_{t+n-1}(S_{t+n})\n",
    "$$\n",
    "* As the value function will be updated at all steps along the way, the update for time step $t$ can use the most recent estimate $V_{t+n-1}$\n",
    "* Should $t+n\\ge T$, then the missing terms are simply treated as $0$, and the $n$-step return is simply the actual sample return\n",
    "* As the $n$-step return is only available after $n$ steps, the update rule looks like the following:\n",
    "\n",
    "$$\n",
    "V_{t+n}(S_t)\\leftarrow V_{t+n-1}(S_t)+\\alpha\\left[G_{t:t+n}-V_{t+n-1}(S_t)\\right]\n",
    "$$\n",
    "* In the expectation, the error of the $n$-step return from state $s$ is less than $\\gamma^n$ times the error of the value estimate $V_{t+n-1}(s)$\n",
    "\n",
    "$$\n",
    "\\underset{s}{\\operatorname{max}}\\biggl|\\mathbb E_\\pi\\left[G_{t:t+n}|S_t=s\\right]-v_\\pi(s)\\biggr|\\le\\gamma^n\\underset{s}{\\operatorname{max}}\\biggl|V_{t+n-1}(s)-v_\\pi(s)\\biggr|\n",
    "$$\n",
    "* This means that the $n$-step TD target is, in expectation, a better estimate of $v_\\pi(s)$ than $V_{t+n-1}(s)$\n",
    "* This is the $error\\ reduction\\ property$ of $n$-step returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49169af-2657-4940-aab5-34a7975d8629",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{7.1}$\n",
    "\n",
    "#### In Chapter 6 we noted that the Monte Carlo error can be written as the sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that the $n$-step error used in (7.2) can also be written as a sum of TD errors (again if the value estimates don’t change) generalizing the earlier result.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_{t:t+n}-V(S_t)&=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)+\\gamma G_{t+1:t+n}-\\gamma V(S_{t+1}) \\\\\n",
    "&=\\delta_t+\\gamma(G_{t+1:t+n}-V(S_{t+1})) \\\\\n",
    "&=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2(G_{t+2:t+n}-V(S_{t+2})) \\\\\n",
    "&=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{n-1}\\delta_{t+n-1} \\\\\n",
    "&=\\sum_{k=t}^{t+n-1}\\gamma^{t-k}\\delta_{k}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7434bea0-310b-438a-b96c-5df4dc9161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NStateRandomWalk:\n",
    "    def __init__(self, n):\n",
    "        assert n // 2 != n / 2\n",
    "        self.n = n\n",
    "        self.reset()\n",
    "        self.values = np.array([\n",
    "            (-(n + 1) + 2 * (i + 1)) / (n + 1) \n",
    "            for i in range(self.n)\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.n // 2\n",
    "        return self.state\n",
    "\n",
    "    def step(self):\n",
    "        d = int(np.sign(np.random.randn()))\n",
    "        self.state += d\n",
    "        if self.state == -1:\n",
    "            self.reset()\n",
    "            return self.state, -1, True\n",
    "        elif self.state == self.n:\n",
    "            self.reset()\n",
    "            return self.state,  1, True\n",
    "        else:\n",
    "            return self.state,  0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4d834cac-b34b-487d-b466-2752d7feeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import inf\n",
    "\n",
    "\n",
    "class NStepStorage:\n",
    "    def __init__(self, n, dtype=float):\n",
    "        self.data = np.zeros(n + 1, dtype=dtype)\n",
    "        self.n = n\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.data[key % (self.n + 1)]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.data[key % (self.n + 1)] = value\n",
    "\n",
    "\n",
    "class NStepTD:\n",
    "    def __init__(self, n, alpha, walk):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.walk = walk\n",
    "        self.V = np.zeros(walk.n)\n",
    "\n",
    "    def train(self, num_episodes=10, quiet=False):\n",
    "        for _ in tqdm(range(num_episodes), disable=quiet):\n",
    "            states = NStepStorage(self.n, dtype=int)\n",
    "            rewards = NStepStorage(self.n)\n",
    "            states[0] = self.walk.reset()\n",
    "            done = False\n",
    "            t = 0\n",
    "            T = inf\n",
    "            while True:\n",
    "                t += 1\n",
    "                if t < T:\n",
    "                    states[t], rewards[t], done = self.walk.step()\n",
    "                    if done:\n",
    "                        T = t\n",
    "                tau = t - self.n\n",
    "                if tau >= 0:\n",
    "                    ret = sum(rewards[step] for step in range(tau + 1, min(T, t) + 1))\n",
    "                    if t < T:\n",
    "                        ret += self.V[states[t]]\n",
    "                    self.V[states[tau]] += self.alpha * (ret - self.V[states[tau]])\n",
    "                if tau == T - 1:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "4d8a1840-b8ae-4e83-ab49-3fa463733e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 10000/10000 [00:22<00:00, 434.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21283498531057776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "walk = NStateRandomWalk(19)\n",
    "\n",
    "rmss = []\n",
    "for _ in tqdm(range(10_000)):\n",
    "    agent = NStepTD(n=4, alpha=0.4, walk=walk)\n",
    "    agent.train(10, quiet=True)\n",
    "    rmss.append(np.sqrt(np.average((agent.V - walk.values)**2)))\n",
    "print(np.average(rmss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1cf28-a9e9-4c45-b05c-08be50bf1a5d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{7.2}\\ (programming)$\n",
    "\n",
    "#### With an n-step method, the value estimates do change from step to step, so an algorithm that used the sum of TD errors (see previous exercise) in place of the error in (7.2) would actually be a slightly different algorithm. Would it be a better algorithm or a worse one? Devise and program a small experiment to answer this question empirically.\n",
    "\n",
    "As can be seen below, this method performs slightly worse than the true $n$-step TD algorithm. This is to be expected, as it essentially uses outdated information instead of the most recent value estimate. If no states were visited multiple times within $n$ steps, the value function for the relevant states would not be updated along the way, and the two methods would be equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a1981f34-57a5-4926-b259-f4d8161151db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepTDAlt:\n",
    "    def __init__(self, n, alpha, walk):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.walk = walk\n",
    "        self.V = np.zeros(walk.n)\n",
    "\n",
    "    def train(self, num_episodes=10, quiet=False):\n",
    "        for _ in tqdm(range(num_episodes), disable=quiet):\n",
    "            states = NStepStorage(self.n, dtype=int)\n",
    "            states[0] = self.walk.reset()\n",
    "            errors = NStepStorage(self.n)\n",
    "            done = False\n",
    "            t = 0\n",
    "            T = inf\n",
    "            while True:\n",
    "                t += 1\n",
    "                if t < T:\n",
    "                    states[t], reward, done = self.walk.step()\n",
    "                    if done:\n",
    "                        T = t\n",
    "                        errors[t-1] = reward - self.V[states[t-1]]\n",
    "                    else:\n",
    "                        errors[t-1] = reward + self.V[states[t]] - self.V[states[t-1]]\n",
    "                tau = t - self.n\n",
    "                if tau >= 0:\n",
    "                    error = 0\n",
    "                    for i in range(tau, min(T, t)):\n",
    "                        error += errors[i]\n",
    "                    self.V[states[tau]] += self.alpha * error\n",
    "                if tau == T - 1:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "9a5d7ad5-5e22-4755-91fe-c02cba87ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 454.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True n-step TD RMS: 0.2177865100402878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 492.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternate n-step TD RMS: 0.24565859843320265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "walk = NStateRandomWalk(19)\n",
    "\n",
    "rmss = []\n",
    "for _ in tqdm(range(1_000)):\n",
    "    agent = NStepTD(n=4, alpha=0.4, walk=walk)\n",
    "    agent.train(10, quiet=True)\n",
    "    rmss.append(np.sqrt(np.average((agent.V - walk.values)**2)))\n",
    "print(f\"True n-step TD RMS: {np.average(rmss)}\")\n",
    "\n",
    "rmss = []\n",
    "for _ in tqdm(range(1_000)):\n",
    "    agent = NStepTDAlt(n=4, alpha=0.4, walk=walk)\n",
    "    agent.train(10, quiet=True)\n",
    "    rmss.append(np.sqrt(np.average((agent.V - walk.values)**2)))\n",
    "print(f\"Alternate n-step TD RMS: {np.average(rmss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad093b-ccff-4b08-9f89-e83388d93ad7",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{7.3}$\n",
    "\n",
    "#### Why do you think a larger random walk task (19 states instead of 5) was used in the examples of this chapter? Would a smaller walk have shifted the advantage to a different value of $n$? How about the change in left-side outcome from $0$ to $-1$ made in the larger walk? Do you think that made any difference in the best value of n?\n",
    "\n",
    "If $n\\ge T$, then the values of all encountered states are updated towards $R_T$, turning it into a constant-$\\alpha$ MC algorithm. The advantages of bootstrapping and learning from every step are lost, and the method becomes less effective. Thus, on the 5-state random walk, an even smaller $n$, probably $n=1$, would have been optimal for faster learning. I don't see how a change in reward from the left-side outcome would make an impact on the optimal choice of $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147bc6f-9c8d-4ff1-9361-b2f2adbc25d1",
   "metadata": {},
   "source": [
    "## $n$-step Sarsa\n",
    "\n",
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
