{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a17b6ff-3f40-43c7-b5a0-d077a171e4ef",
   "metadata": {},
   "source": [
    "# Day 4 - Debrief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01663a-df87-4c15-a857-e35514030d9c",
   "metadata": {},
   "source": [
    "## What I did today\n",
    "\n",
    "I worked through the entire third chapter of Sutton & Barto, about finite MDPs, including all 29 exercises, and watched David Silver's second lecture, covering that same topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006255c-ac50-4f60-a4de-5ca8dbf2b5cb",
   "metadata": {},
   "source": [
    "## What I learned today\n",
    "\n",
    "I learned about the MDP formalism, defining a decision process by an environment and its dynamics, as well as an agent with a policy for taking specific actions in each of the environment's states, receiving rewards in the process.\n",
    "\n",
    "I learned about the goal of RL, which is to maximize the expected future reward received, and how value functions and their Bellman equations define this objective.\n",
    "\n",
    "I also learned how the Bellman optimality equations relate to the optimal policy, which maximizes the expected reward received by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389843fe-26a9-4469-85a6-be0163f6bbe7",
   "metadata": {},
   "source": [
    "## Notes and questions for the next day\n",
    "\n",
    "How can an agent learn when, and for how long, to think through something, before acting. How can the reward be propagated? Is this perhaps already solved by normal backprop, or do concepts like eligibility traces play a role?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
