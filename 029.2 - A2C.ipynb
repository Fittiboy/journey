{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae1d4ce-1d61-4e11-b647-b77b8d1bdb64",
   "metadata": {},
   "source": [
    "# Day 29 - Advantage Actor-Critic (A2C) Algorithm: Theory and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cec2d-9b0c-46f5-8aae-fe6bb7f0404d",
   "metadata": {},
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0bf8e-c03e-4d7e-9c5f-e6d1083c8690",
   "metadata": {},
   "source": [
    "* The purely value-based DQN is sample-inefficient, and only deals with\n",
    "  discrete action spaces\n",
    "* The purely policy-based REINFORCE has high variance, and updates only\n",
    "  after finishing an episode\n",
    "* Actor-Critic methods combine the best of both worlds, where a policy\n",
    "  is judged with respect to a baseline value learned by the critic\n",
    "* This leads to lower variance, higher sample-efficiency, and applicability\n",
    "  to continuous action spaces and stochastic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73347a-37ca-4ebe-8c64-2c85d77f90cb",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e03a2-1281-407e-94d5-bd64c10de1f6",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic and Baseline (Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02c3f5-df56-4015-a790-50d547fcaa04",
   "metadata": {},
   "source": [
    "To reduce variance, the A2C algorithm learns the state value function as a baseline,\n",
    "defining the advantage function as\n",
    "$$\n",
    "A_\\pi(s,a)=Q_\\pi(s,a)-V_\\pi(s).\n",
    "$$\n",
    "This value represents a measure of how much greater the reward for choosing action\n",
    "$a$ is, than following the current policy.\n",
    "As the state value is independent of the policy, we can subtract it before taking the gradient,\n",
    "allowing us to substitute the advantage for the action value:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb E_{\\pi_\\theta}\\left[\n",
    "\\nabla_\\theta\\operatorname{log}\\pi_\\theta(a|s)A_\\pi(s,a)\\right]\n",
    "$$\n",
    "One important change this brings is that instead of always making the chosen action more likely,\n",
    "based on the return, it *reduces* the likelihood of an action being chosen again, if the advantage\n",
    "was negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26364a1e-4987-4cea-908c-ac0fcb370195",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning and TD Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc184223-a85f-4f83-842d-cca408418dbe",
   "metadata": {},
   "source": [
    "The value estimate is updated via TD learning.\n",
    "The TD error $\\delta$ serves as an effective estimator of the value function, so that\n",
    "no action value estimator has to be learned. (But what if we do that anyway? Actually,\n",
    "that would probably lead to them no longer being identifiable.)\n",
    "\n",
    "Additionally, A2C often uses $n$-step returns for this update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553097b-552f-4211-bbdb-08f45291cfc1",
   "metadata": {},
   "source": [
    "### Summary of A2C Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c53848-de7e-4b40-add3-9c77b2d71551",
   "metadata": {},
   "source": [
    "1. Actor (Policy) Update: We replace the return with the TD error\n",
    "2. Critic (Value) Update: We minimize the TD error\n",
    "3. This is done as a combined update, minimizing $L = L_{\\text{actor}}+ c\\cdot L_{\\text{critic}}$,\n",
    "   where $c$ is a scaling factor, to control relative update sizes\n",
    "4. Adding an entropy bonus, $L_{\\text{entropy}} = -\\beta\\cdot\\mathcal{H}(\\pi(s))$, allows us to\n",
    "   control exploration by changing $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba13717-2451-44e7-8c82-4ac3fb320f9d",
   "metadata": {},
   "source": [
    "## Implementation Mechanics of A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06020022-8bbf-4470-a349-048befa49fa9",
   "metadata": {},
   "source": [
    "High-level overview of A2C:\n",
    "1. Initialize actor and critic\n",
    "2. Collect experience\n",
    "3. Calculate the $n$-step TD error\n",
    "4. Compute actor and critic losses\n",
    "5. Optionally aggregate losses over a batch, or sum them\n",
    "6. Perform the parameter update\n",
    "7. Goto 1 until happy\n",
    "\n",
    "A2C is often parallelized with multiple environments, but I'm working towards fully autonomous\n",
    "real-world agents that should not be required to learn in tandem, so we will not implement this\n",
    "here.\n",
    "\n",
    "Implementation tips:\n",
    "* If unstable, batch updates and introduce multi-step returns\n",
    "* Carefully tune either separate learning rates, or the loss balancing\n",
    "  factor $c$\n",
    "* Clip gradients to prevent large updates from destabilizing training\n",
    "* Clip or normalize rewards if they destabilize training\n",
    "* Decay $\\beta$ to encourage more exploration mostly during early training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186e49-62a3-4daa-bddd-94acd3710c18",
   "metadata": {},
   "source": [
    "## Implementing A2C from Scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac7a70-ef0d-4d79-b25a-7639998061ef",
   "metadata": {},
   "source": [
    "### Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b350f1c9-d5cf-419b-955b-6cc53ce6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5412cfc1-0e0b-4264-8773-96c4bfde43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ec9a3e-4be2-4a4a-b24c-eead72d50e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f3f18d-7e73-4bb3-82ff-c9aff4a0f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"ALE/Breakout-v5\"\n",
    "project_name = \"BREAKOUT-A2C\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "video_folder = \"./videos/\" + project_name + timestamp\n",
    "video_frequency = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866d6ee0-de08-4b2e-9e1f-7491726877ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "learning_rate = 1e-4\n",
    "entropy_coeff = 2e-3\n",
    "critic_coeff = 0.5\n",
    "\n",
    "config = {\n",
    "    \"env\": env_name,\n",
    "    \"algo\": \"A2C\",\n",
    "    \"gamma\": gamma,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"entropy_coeff\": entropy_coeff,\n",
    "    \"critic_coeff\": critic_coeff,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce033ad-d0e1-4cc7-b3b6-275909a32651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfitti\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/journey/wandb/run-20250213_172732-v5rgvpf9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9' target=\"_blank\">silver-sun-46</a></strong> to <a href='https://wandb.ai/fitti/BREAKOUT-A2C' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/BREAKOUT-A2C' target=\"_blank\">https://wandb.ai/fitti/BREAKOUT-A2C</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9' target=\"_blank\">https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7dd58c6599d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da5a727-0ea3-44d8-b594-f3891cd41ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Box(0.0, 1.0, (4, 84, 84), float32), Discrete(4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env=env,\n",
    "    video_folder=video_folder,\n",
    "    episode_trigger=lambda x: x % video_frequency == 0,\n",
    ")\n",
    "env = gym.wrappers.AtariPreprocessing(\n",
    "    env=env,\n",
    "    scale_obs=True,\n",
    ")\n",
    "env = gym.wrappers.FrameStackObservation(env=env, stack_size=4)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df53ae-1d60-4def-8d9e-d92abcdbe3e3",
   "metadata": {},
   "source": [
    "### Define the Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9208ac4-1f70-4d46-ade8-c06fb7ef1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers, for four stacked grayscale 84x84 frames\n",
    "        self.conv1 = nn.Conv2d(in_channels= 4, out_channels=16, kernel_size=5, stride=2) # 16 x 40 x 40\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2) # 32 x 18 x 18\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2) # 32 x 7 x 7\n",
    "        conv_output_size = 32 * 7 * 7\n",
    "\n",
    "        # Fully connected heads\n",
    "        self.actor = nn.Linear(conv_output_size, n_actions)\n",
    "        self.critic = nn.Linear(conv_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00574cf8-1931-4e4d-8f65-664ebfd4bca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (actor): Linear(in_features=1568, out_features=4, bias=True)\n",
       "  (critic): Linear(in_features=1568, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ActorCritic(n_actions).to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "wandb.watch(net, log=\"all\", log_freq=50)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05832044-b614-49f1-becf-99a8e4592ffd",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0247ecaa-ad19-4001-8836-3328de59fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06a94e30-a87a-4d93-9ee9-afca72595e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(logits):\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    entropy = dist.entropy()\n",
    "\n",
    "    return action.item(), log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd40622e-31d7-47b1-9ff8-12ab213a8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net: nn.Module,\n",
    "    env: gym.Env,\n",
    "    optimizer: optim.Optimizer,\n",
    "    entropy_coeff: float,\n",
    "    critic_coeff: float,\n",
    "    n_episodes: int,\n",
    "):\n",
    "    try:\n",
    "        for episode in tqdm(range(1, n_episodes + 1), desc=\"Episodes\"):\n",
    "            obs, _ = env.reset()\n",
    "            state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "            episode_reward = 0\n",
    "            total_actor_loss = 0\n",
    "            total_critic_loss = 0\n",
    "            total_entropy_loss = 0\n",
    "    \n",
    "            done, truncated = False, False\n",
    "            t = 0\n",
    "            while not (done or truncated):\n",
    "                t += 1\n",
    "                logits, value = net(state)\n",
    "                action, log_prob, entropy = select_action(logits)\n",
    "                obs, reward, done, truncated, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "                if not done:\n",
    "                    _, next_value = net(state)\n",
    "                else:\n",
    "                    next_value = torch.tensor([[0.0]], device=device)\n",
    "\n",
    "                reward = torch.tensor(reward, device=device)\n",
    "                td_target = reward + next_value.squeeze()\n",
    "                td_error = td_target - value\n",
    "\n",
    "                actor_loss = -log_prob * td_error.detach()\n",
    "                critic_loss = critic_coeff * td_error.pow(2)\n",
    "                entropy_loss = entropy_coeff * -entropy\n",
    "                loss = actor_loss + critic_loss + entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_actor_loss += actor_loss.detach()\n",
    "                total_critic_loss += critic_loss.detach()\n",
    "                total_entropy_loss += entropy_loss.detach()\n",
    "\n",
    "            avg_actor_loss = total_actor_loss / t\n",
    "            avg_critic_loss = total_critic_loss / t\n",
    "            avg_entropy_loss = total_entropy_loss / t\n",
    "            avg_loss = avg_actor_loss + avg_critic_loss + avg_entropy_loss\n",
    "    \n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"actor_loss\": avg_actor_loss.item(),\n",
    "                    \"critic_loss\": avg_critic_loss.item(),\n",
    "                    \"entropy_loss\": avg_entropy_loss.item(),\n",
    "                    \"loss\": avg_loss.item(),\n",
    "                    \"episode_reward\": episode_reward,\n",
    "                })\n",
    "    \n",
    "            if episode % video_frequency == 0:\n",
    "                latest_video = max(\n",
    "                        Path(video_folder).iterdir(),\n",
    "                        key=lambda x: x.stat().st_mtime\n",
    "                    )\n",
    "                wandb.log({\n",
    "                    \"video\": wandb.Video(str(latest_video))\n",
    "                })\n",
    "    \n",
    "                print(\n",
    "                    f\"Episode {episode}:\",\n",
    "                    f\"Return: {episode_reward}\",\n",
    "                    f\"Average loss: {avg_loss.item():.4f}\",\n",
    "                    end=\"\\t\\t\\r\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining stopped manually.\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5003e9ec-563d-4415-af17-1f47691a968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3c05a6e92943b4a033823ddb041758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000: Return: 0.0 Average loss: -0.0052\t\t"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>▇▂█▅▅▅▅▇▂▂▆▂▅▁▂▅▁▂▅▃▁▃▁▂▃▁▁▃▃▄▃▁▃▃▂▃▁▃▃▃</td></tr><tr><td>critic_loss</td><td>▇▄▄▄▁▃▃▁▂▁▂▁█▁▃▁▂▃▃▁▁▂▁▂▁▁▂▂▁▂▁▁▁▁▁▇▂▁▁▁</td></tr><tr><td>entropy_loss</td><td>▁▁▁▁▁▁▂▂▃▃▁▃▄▆█▁▁▁▅▄▃▅▁▁▅▁▁▄▁▅▁▆▁▄▄▁▄▄▁█</td></tr><tr><td>episode_reward</td><td>▄▁▅▃▁▂▃▃█▁▁▂▃▇▄▁▂▁▂▁▆▂▂▁▂▂▂▃▁▂▁▃▆▁▁▁▃▂▆▂</td></tr><tr><td>loss</td><td>▄▆▇▇▁▅▅▆▅▁▁▄▂▅▃█▃▁▁▁▄▃▂▁▅▁▁▂▃▁▁▁▃▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>-0.00252</td></tr><tr><td>critic_loss</td><td>4e-05</td></tr><tr><td>entropy_loss</td><td>-0.00276</td></tr><tr><td>episode_reward</td><td>0</td></tr><tr><td>loss</td><td>-0.00524</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sun-46</strong> at: <a href='https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9' target=\"_blank\">https://wandb.ai/fitti/BREAKOUT-A2C/runs/v5rgvpf9</a><br> View project at: <a href='https://wandb.ai/fitti/BREAKOUT-A2C' target=\"_blank\">https://wandb.ai/fitti/BREAKOUT-A2C</a><br>Synced 5 W&B file(s), 200 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250213_172732-v5rgvpf9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(net, env, optimizer, entropy_coeff, critic_coeff, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638e1dc-0166-458e-a47e-3b7500a09162",
   "metadata": {},
   "source": [
    "I had to go on quite a debugging journey, trying to figure out why autograd complained\n",
    "that a value had changed. The reason was that I have to recompute the value for the\n",
    "next state at the beginning of each step, despite already having computed this value\n",
    "on the previous step. As the network has changed in the meantime, that value is no\n",
    "longer valid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
