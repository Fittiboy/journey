{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "764f7cd2-752f-45cc-9d13-7259fcaf78a5",
   "metadata": {},
   "source": [
    "# Day 1 - Multi-armed Bandits\n",
    "\n",
    "$Part\\ I$ of [the book](http://incompleteideas.net/book/RLbook2020.pdf) is divided into three subsections:\n",
    "1. The first and second chapter, addressing the simplest case, multi-armed bandits, and the general problem formulation, finite Markov decision processes.\n",
    "2. The following three chapters, introducting dynamic programming, Monte Carlo methods, and temporal-difference learning.\n",
    "3. The final two chapters, addressing how these three methods can be combined to achieve more and more powerful algorithms.\n",
    "\n",
    "* RL is distinguished by actions being evaluated\n",
    "* RL uses evaluative feedback, rating the action taken, without giving information on whether it was the best action\n",
    "* Supervised learning gives only the best action, without rating the chosen action (instructive feedback)\n",
    "* This chapter treats learning in only a single situation, the *nonassociative* setting\n",
    "* The specific problem is a simple $k$-armed bandit problem\n",
    "* At the end of the chapter, we take a step to full RL by considering the *associative* setting of multiple different situations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f34cbad0-9618-447d-b5b8-3c59ae822c3e",
   "metadata": {},
   "source": [
    "## A $k$-armed Bandit Problem\n",
    "\n",
    "* The $k$-armed bandit is a simple problem where you pick from a set of actions over and over again\n",
    "* Each time you select an action, you receive a reward from some distribution\n",
    "* The goal is to maximize rewards over some number of time steps\n",
    "* Each of the $k$ actions have some expected/mean reward, which we call the $value$ of the action\n",
    "* The action selected at time step $t$ is $A_t$, the reward received $R_t$\n",
    "* The value of an action $a$ is: $$q_*(a)\\doteq\\mathbb E\\left[R_t|A_t=a\\right]$$\n",
    "* Our estimate at time $t$ of this value is $Q_t(a)$\n",
    "* As we keep an estimate, there's always a greatest value at each time step, which is the $greedy$ action\n",
    "* Picking the greedy action is $exploitation$ of our knowledge\n",
    "* Picking any nongreedy action is $exploration$ to gain more knowledge\n",
    "* If you have many time steps left, exploration may be better for long term reward\n",
    "* Methods for finding the optimal solution to the exploration/exploitation trade-off usually do not apply to real RL problems\n",
    "* We try to balance this, but do not care about doing so in a sophisticated way\n",
    "* Methods involving exploration work much better than purely greedy action selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6203aa9a-6459-4d6e-aac4-7b740bb8d9f7",
   "metadata": {},
   "source": [
    "## Action-value Methods\n",
    "\n",
    "* $Action-value\\ methods$ use estimates of $q_*(a)$ to make decisions\n",
    "* One way to estimate these is to average the rewards received: $$Q_t(a)\\doteq\\frac{\\text{sum of rewards when }a\\text{ taken prior to }t}{\\text{number of times }a\\text{ taken prior to }t}=\\frac{\\sum_{i=1}^{t-1}R_i\\cdot\\mathbb 1_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb 1_{A_i=a}}$$\n",
    "* If the action $a$ has not yet been taken, $Q_t(a)$ has some default value, like 0\n",
    "* As the denominator goes to infinity, $Q_t(a)$ converges to $q_*(a)$\n",
    "* This is called the $sample$-$average\\ method$ for estimating action values\n",
    "* $Greedy$ action-selection is written as $$A_t\\doteq\\underset{a}{\\operatorname{arg max}}Q_t(a)$$\n",
    "* A simple alternative is $\\varepsilon$-$greedy$ action-selection, where a random action is taken with a probability of $\\varepsilon$, the greedy action taken otherwise\n",
    "* This ensures that each action is taken an infinite number of times, so the $Q_t(a)$ converge to their respective $q_*(a)$\n",
    "* This assumes that the probability of selecting greedy action converges to near certainty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3c5dab6-8ffb-4da8-ae6f-aa09872793e1",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{2.1}$\n",
    "\n",
    "#### In $\\varepsilon$-greedy action selection, for the case of two actions and $\\varepsilon = 0.5$, what is the probability that the greedy action is selected?\n",
    "\n",
    "Irrespective of the number of actions taken, the probability that the greedy action is selected is always $1-\\varepsilon$, which in this case is $1-0.5=0.5$, so $50\\%$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4583bdce-0012-45b6-ae1f-8f0f111cf965",
   "metadata": {},
   "source": [
    "## The 10-armed Testbed\n",
    "\n",
    "* An experiment with 2000 randomly generated $k$-armed bandit problems with $k=10$ arms was performed\n",
    "* The action values $q_*(a),a=1,...,10$ were sampled from a normal distribution with $\\mu=0, \\sigma^2=1$\n",
    "* Actual reward $R_t$ received at time step $t$ after selecting action $A_t$ was sampled from a normal distribution with $\\mu=q_*(A_t), \\sigma^2=1$\n",
    "* Performance was measured over 1000 time steps, constituting one $run$, over a total of 2000 independent runs on different bandit problems\n",
    "* Performance was compared between the greedy method, $\\varepsilon$-greedy with $\\varepsilon=0.1$, as well as $\\varepsilon$-greed with $\\varepsilon=0.01$\n",
    "* The greedy method immediately reached an average reward of 1, while the $\\varepsilon=0.1$ method quickly found a reward closer to 1.5, which the $\\varepsilon=0.01$ method approached more slowly\n",
    "* The greedy method selected the optimal action in only about a third of the runs, while $\\varepsilon=0.1$ approached about $80\\%$, and $\\varepsilon=0.01$ about $50\\%$ of the time\n",
    "* Performance could improve even further if $\\varepsilon$ is reduced over time\n",
    "* Environments with noisier rewards favor exploration even more heavily\n",
    "* The same is true for the deterministic case with nonstationary rewards, as the previously optimal action may suddenly no longer be optimal\n",
    "* Nonstationarity is the most common case in RL, as the agent learns and improves its policy over time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30076826-f15c-45e0-b449-decdd6891565",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{2.2}$*:*$\\ Bandit\\ example$\n",
    "\n",
    "#### Consider a $k$-armed bandit problem with $k = 4$ actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\\varepsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a)=0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1=1,\\ R_1=-1,\\ A_2=2,\\ R_2=1,\\ A_3=2,\\ R_3=-2,\\ A_4=2,\\ R_4=2,\\ A_5=3,\\ R_5=0$. On some of these time steps the $\\varepsilon$ case may have occurred, causing an action to be selected at random.\n",
    "\n",
    "$t=1:\\; Q_2(1)=R_1=-1$, all others $0$, so $2$-$4$ are greedy  \n",
    "$t=2:\\; Q_3(2)=R_2=1$, so $2$ is greedy  \n",
    "$t=3:\\; Q_4(3)=R_3=-2$, so $2$ is greedy  \n",
    "$t=4:\\; Q_5(2)=\\frac{R_2+R_4}{2}=\\frac{1+2}{2}=1.5$, so $2$ is greedy  \n",
    "$t=5:\\; Q_6(3)=\\frac{R_3+R_5}{2}=\\frac{-2+0}{2}=-1$, so $2$ is greedy\n",
    "\n",
    "#### On which time steps did this definitely occur?\n",
    "\n",
    "Starting after $t=2$, the greedy action is always $2$, but $3$ is chosen for $t=3$ and $t=5$, meaning those were definitely exploratory actions.\n",
    "\n",
    "#### On which time steps could this possibly have occurred?\n",
    "\n",
    "As the $\\varepsilon$-greedy policy can choose the greedy action even at random, an action could have been selected at random at each time step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0954aba-8864-4264-9413-0ae5a80de39d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{2.2}$\n",
    "\n",
    "#### In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action?\n",
    "\n",
    "In the long run, the method using $\\varepsilon=0.01$ will perform the best.\n",
    "\n",
    "#### How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "Both $\\varepsilon$-greedy policies will find the optimal action, but the $\\varepsilon=0.01$ method will choose the optimal action $99.1\\%$ of the time, while the $\\varepsilon=0.1$ method only does so $91\\%$ of the time. The book claims the best possible value to be $1.54$, while we know the average reward to be $0$. So the $\\varepsilon=0.01$ method receives an average reward of about $1.53$ per time step, while the $\\varepsilon=0.1$ method receives only about $1.40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ecb838-b637-4708-baf6-b87e4619fd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.52614, 1.4014)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.991 * 1.54, 0.91 * 1.54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
