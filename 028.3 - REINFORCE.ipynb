{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8346b3-d34f-4844-9c69-35d0986d9c6d",
   "metadata": {},
   "source": [
    "# Day 28 - REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899ca8c-98a7-4e6f-92ca-3c774504cad9",
   "metadata": {},
   "source": [
    "Following this [guide](https://chatgpt.com/share/67acc02c-e5f4-800e-9129-899c74684e09)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29dba9-24f0-49e0-ab08-87317a5fa1e0",
   "metadata": {},
   "source": [
    "## Intuition Behind Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d938b-d824-4bed-bbca-de7835db8e66",
   "metadata": {},
   "source": [
    "* Directly learning a policy allows an agent to handle continuous action spaces naturally\n",
    "* Policy gradient methods are able to learn stochastic policies explicitly\n",
    "* Policy estimation is the simplest, most direct form of RL, as it directly learns the final goal: The policy\n",
    "* REINFORCE uses Monte Carlo returns, which can avoid bootstrapping stability issues,\n",
    "  but can be quite slow and introduce a lot of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75095b51-60e2-4e03-8f00-f7285f016b5a",
   "metadata": {},
   "source": [
    "## Mathematical Foundation: Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be5d90-8feb-4754-bccd-d03dcb692300",
   "metadata": {},
   "source": [
    "* To understand policy gradient methods, we have to understand how to compute the gradient\n",
    "  of the policy's performance\n",
    "* The performance is, of course, the expected return\n",
    "$$\n",
    "J(\\theta)=\\mathbb E_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=1}^T\\gamma^{t-1}r_{t}\\right],\n",
    "$$\n",
    "  where $\\tau$ is a trajectory $(s_0, a_0, r_1, s_1, \\dots)$\n",
    "* Our goal is then to maximize this expectation\n",
    "* So we are now looking for $\\nabla_\\theta J(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\nabla_\\theta\\int P(\\tau;\\theta)R(\\tau)d\\tau=\\int\\nabla_\\theta P(\\tau;\\theta)R(\\tau)d\\tau\n",
    "$$\n",
    "\n",
    "* We can use the log-likelihood trick, taking advantage of the identity\n",
    "  $\\nabla_{\\theta} P(\\tau;\\theta) = P(\\tau;\\theta),\\nabla_{\\theta} \\log P(\\tau;\\theta)$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\\int P(\\tau;\\theta)\\nabla_\\theta \\operatorname{log}P(\\tau;\\theta)\n",
    "R(\\tau)d\\tau=\\mathbb E_{\\tau\\sim\\pi_\\theta}\\left[\\nabla_\\theta \\operatorname{log}P(\\tau;\\theta)R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "* Luckily, we know that $P(\\tau;\\theta)=\\prod_{t=0}^T\\pi(a|s)p(s',r'|s,a)$, and that $p(s',r'|s,a)$\n",
    "  does not depend on $\\theta$, so that factor is a constant in the gradient\n",
    "* Substituting this yield a very simple expression which we can estimate from experience:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta\\operatorname{log}P(\\tau;\\theta)=\\sum_{t=0}^T\\nabla_\\theta\\operatorname{log}\\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "* So, the policy gradient now looks like this:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb E_{\\tau\\sim\\pi_\\theta}\\left[\n",
    "\\sum_{t=0}^T\\nabla_\\theta\\operatorname{log}\\pi_\\theta(a_t|s_t)R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "* In this expectation, as $R(\\tau)$ is the return of the trajectory, we can replace it with the\n",
    "  expected return, which is $Q_{\\pi_\\theta}(s_t,a_t)$, with $t=0$ for the entire trajectory, giving us the final\n",
    "  policy gradient theorem:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb E_{\\pi_\\theta}\\Bigl[\n",
    "\\nabla_\\theta\\operatorname{log}\\pi_\\theta(a|s)Q_{\\pi_\\theta}(s,a)\\Bigr]\n",
    "$$\n",
    "\n",
    "* The sum goes missing here, because the summed term is the same for each $t$, so it is just $T+1$ times that\n",
    "  term, which is a factor that can be silently absorbed into the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503e7ae-1984-4fb1-9520-d7ff8c58b874",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm (Monte Carlo Policy Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcba661-95cc-4f8d-b8e1-f68713b74f05",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm is simple:\n",
    "1. Sample episodes using the current policy.\n",
    "2. Adjust the policy parameter $\\theta$ in the direction of $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$,\n",
    "   where $G_t$ is the sampled return, for each time step $t$ of the episode.\n",
    "$$\n",
    "\\theta\\leftarrow\\theta+\\alpha G_t\\nabla_\\theta\\operatorname{log}\\pi(a_t|s_t)\n",
    "$$\n",
    "   In practice, this update is done as an average, or a sum over all timesteps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
