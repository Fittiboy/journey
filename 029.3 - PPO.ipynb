{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c184634e-58bd-4baa-8d7e-ccaef2a6b2d2",
   "metadata": {},
   "source": [
    "# Day 29 - Proximal Policy Optimization with Clipping (PPO-Clip): Theory and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561838b-24f3-4948-b63b-0ea06783897b",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af746d3e-ac24-4be8-9d99-0f26c5726363",
   "metadata": {},
   "source": [
    "### Motivation: From Policy Gradients to TRPO to PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca189c-d963-4c31-a190-8ca0c6717f9d",
   "metadata": {},
   "source": [
    "* Sometimes, large policy updates can destroy useful behavior the agent has previously\n",
    "  learned\n",
    "* TRPO was introduced to stop the policy from deviating too far in a single update\n",
    "* This requires complex computations in practice, and may be difficult to tune in practice\n",
    "* PPO simplifies the idea of constraining the policy update, making it much simpler to implement\n",
    "  while being as good, and sometimes better than, TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c097a-0f93-40a9-b7d3-6c1dc37d136b",
   "metadata": {},
   "source": [
    "### Trust Region Constraint vs. Clipped Objective in PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5061a8d-44ec-4109-8898-bfb6bdb1ceed",
   "metadata": {},
   "source": [
    "Both TRPO and PPO make use of the probability ratio of an action under the new and\n",
    "under the old policy:\n",
    "$$\n",
    "r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)}\n",
    "$$\n",
    "TRPO uses this ratio in the surrogate objective, while constraining the average KL divergence of the new policy from the old policy. PPO uses this ratio directly in a clipped objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc09c2-1c74-467f-9392-130e3109c3a1",
   "metadata": {},
   "source": [
    "### PPO-Clip Objective Function Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb521c6-66c9-4c00-ad9b-33681cfa190a",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{\\text{CLIP}}(\\theta)=\\mathbb E\\left[\\operatorname{min}(r_t(\\theta)\\hat{A}_t,\\,\n",
    "\\operatorname{clip}\\left(r_t(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\hat{A}_t)\\right]\n",
    "$$\n",
    "As the policy is updated in batches, this prevents any single batch from changing the policy too far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
