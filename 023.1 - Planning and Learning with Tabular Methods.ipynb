{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03aeda0f-21d5-41b0-8843-106771125795",
   "metadata": {},
   "source": [
    "# Day 23 - Planning and Learning with Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d3675-ba25-4cda-9922-7873b5d9c3fc",
   "metadata": {},
   "source": [
    "## Trajectory Sampling\n",
    "\n",
    "* We look at the different ways of distributing updates\n",
    "* The classical approach is to sweep through the entire state space\n",
    "* The second approach is to sample according to some distribution\n",
    "* Sampling individual trajectories according to, for example, the on-policy distribution, is called $trajectory\\ sampling$\n",
    "* This can be beneficial, as it can ignore the uninteresting parts of the state space\n",
    "* It may also ignore important parts of the state space, which the policy is missing, however\n",
    "* It turns out that, on the large problems we care aboue, on-policy sampling seems generally stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d396f-dbe3-4a65-abde-31105a3bdcc2",
   "metadata": {},
   "source": [
    "## Real-Time Dynamic Programming\n",
    "\n",
    "* RTDP is an example of $asynchronous\\ DP$, where the values to be updated are chosen by the current policy\n",
    "* As a form of DP, it is possible to apply some of the theoretical results of DP to RTDP\n",
    "* RTDP can skip regions of the states that are irrelevant to optimal policies\n",
    "* All that is required is an $optimal\\ partial\\ policy$, which is a policy that is optimal in all states that a full optimal policy would visit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c94933-ef32-45aa-9003-fe658e05dcc9",
   "metadata": {},
   "source": [
    "## Planning at Decision Time\n",
    "\n",
    "* Aside from $background\\ planning$, the method we have done so far, where simulated experience is used to update arbitrary value estimates, there is also $decision$-$time\\ planning$\n",
    "* Planning at decisin time means simulating trajectories from $S_t$, to figure out which action would yield the highest value\n",
    "* When time for deliberation is available, this is a strong method\n",
    "* When fast reactions are important, background planning is best to produce a policy that can be immediately applied at all times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233710f-7052-47a4-bf96-03e2fa5de551",
   "metadata": {},
   "source": [
    "## Heuristic Search\n",
    "\n",
    "* $Heuristic\\ search$ builds a tree, strating from the state to be considered, and then backs up computation of values from the value estimates of the leaf nodes\n",
    "* If our estimates are imperfect, but we have a perfect model of the environment, then deeper search leads to better policies\n",
    "* However, this of course requires more and more computation\n",
    "* The success of heuristic search methods is likely due to its tight focus on relevant states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d917ef0-c7e4-4d37-999e-6ee83dea9df2",
   "metadata": {},
   "source": [
    "## Rollout Algorithms\n",
    "\n",
    "* A rollout is a simulated trajectory, starting from the current state, picking a specific action\n",
    "* The values of many samples trajectories for each action are averaged, and the highest-value action is selected to be executed\n",
    "* Then, more rollouts are performed from the next state\n",
    "* The values computed are the values for the so-called $rollout\\ policy$\n",
    "* As the Monte Carlo samples are independently sampled, this can be done in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5a631-4325-4966-9b5d-a005fa1ffd37",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "* MCTS is an algorithm that is run at each new state that is encountered\n",
    "* It builds a tree by iterating a four-step process until it needs to make a decision\n",
    "    1. Selection: Select the most promising child nodes via the tree policy\n",
    "    2. Expansion: New child nodes are added to the selected leaf node\n",
    "    3. Simulation: A simulated trajectory is run to a terminal state, from the expanded node, using the rollout policy\n",
    "    4. Backup: The result of the Monte Carlo sample is propagated back through the nodes in the tree, updating their values\n",
    "* Once it has to make a decision, it chooses an action based on the statistic accumulated\n",
    "* This might either be the action with the largest value, or the one that was visited most often, to avoid outliers\n",
    "* AlphaGo combines this with keeping value estimates learned by a deep neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
