{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72ca900-f168-463d-8dc1-322368a80e48",
   "metadata": {},
   "source": [
    "# Day 8 - Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37a361-7bfd-4cb7-8385-6445e4787a46",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation of Action Values\n",
    "\n",
    "* Without a model of the environment, state-values are not enough to suggest a policy, so we must estimate $q_\\pi(s,a)$\n",
    "* Goal: Estimate $q_*$\n",
    "* The \"visits\" concept transfers from states $s$ to state-action pairs $s,a$\n",
    "* A deterministic policy only ever picks one action $a$ in each state, so estimates of other actions cannot be made\n",
    "* To do so, we need to $maintain\\ exploration$\n",
    "* One way to do so is $exploring\\ starts$, where each episode starts with some state-action pair, and each pair has a nonzero probability of being selected\n",
    "* More practical is to only consider policies that will select each action in each state with a nonzero probability forever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba286b0c-92a4-4e6c-a6df-f6a10b5c210d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.3}$\n",
    "\n",
    "#### What is the backup diagram for Monte Carlo estimation of $q_\\pi$?\n",
    "\n",
    "It starts at a state-action pair, and keeps going until reaching the terminal state of an episode, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cc61b8-b7b0-4fcf-bdf9-bacf42ba360d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 62.00 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-400 58,-400 58,4 -4,4\"/>\n",
       "<!-- a1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"black\" stroke=\"black\" cx=\"27\" cy=\"-378\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- s1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>s1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-306\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;s1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-359.7C27,-352.41 27,-343.73 27,-335.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-335.62 27,-325.62 23.5,-335.62 30.5,-335.62\"/>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"black\" stroke=\"black\" cx=\"27\" cy=\"-234\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;a2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>s1&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-287.7C27,-280.41 27,-271.73 27,-263.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-263.62 27,-253.62 23.5,-263.62 30.5,-263.62\"/>\n",
       "</g>\n",
       "<!-- s2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>s2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;s2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>a2&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-215.7C27,-208.41 27,-199.73 27,-191.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-191.62 27,-181.62 23.5,-191.62 30.5,-191.62\"/>\n",
       "</g>\n",
       "<!-- a3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>a3</title>\n",
       "<ellipse fill=\"black\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;a3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>s2&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-143.7C27,-136.41 27,-127.73 27,-119.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-119.62 27,-109.62 23.5,-119.62 30.5,-119.62\"/>\n",
       "</g>\n",
       "<!-- t -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>t</title>\n",
       "<polygon fill=\"gray\" stroke=\"gray\" points=\"54,-36 0,-36 0,0 54,0 54,-36\"/>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;t -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>a3&#45;&gt;t</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-71.7C27,-64.41 27,-55.73 27,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-47.62 27,-37.62 23.5,-47.62 30.5,-47.62\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x765f341520c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "\n",
    "dot.node('a1',  '', shape='circle', style='filled', color='black')\n",
    "dot.node('s1', '', shape='circle')\n",
    "dot.node('a2',  '', shape='circle', style='filled', color='black')\n",
    "dot.node('s2', '', shape='circle')\n",
    "dot.node('a3',  '', shape='circle', style='filled', color='black')\n",
    "dot.node('t', '', shape='box', style='filled', color='gray')\n",
    "\n",
    "dot.edge('a1', 's1')\n",
    "dot.edge('s1', 'a2')\n",
    "dot.edge('a2', 's2')\n",
    "dot.edge('s2', 'a3')\n",
    "dot.edge('a3', 't')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431d21a-be86-4c1c-b70c-c596d0d42883",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "* To extend Monte Carlo methods to control, we simply have to perform generalized policy iteration with our Monte Carlo estimates\n",
    "* With infinitely many generated episodes and exploring starts, Monte Carlo methods compute $q_{\\pi_k}$ exactly for any policy $\\pi_k$\n",
    "* Then, we can perform policy improvement, by selecting $\\pi_{k+1}(s)=\\operatorname{arg}\\underset{a}{\\operatorname{max}}q_{\\pi_k}(s,a)$, from which follows that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{\\pi_k}(s,\\pi_{k+1}(s))&=q_{\\pi_k}(s,\\operatorname{arg}\\underset{a}{\\operatorname{max}}q_{\\pi_k}(s,a)) \\\\\n",
    "&=\\underset{a}{\\operatorname{max}}q_{\\pi_k}(s,a) \\\\\n",
    "&\\ge q_{\\pi_k}(s,\\pi_k(s)) \\\\\n",
    "&=v_{\\pi_k}(s).\n",
    "\\end{align}\n",
    "$$\n",
    "* As our assumptions are unrealistic, we have to remove them for a practical algorithm\n",
    "* To avoid having to sample infinitely many episodes, we can simply perform updates of both $q$ after each episode, for all states visited, and then update $\\pi$\n",
    "* Convergence is not proven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4a45c-8ef4-4a1e-b3b3-03bdf28ea8d7",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.4}$\n",
    "\n",
    "#### The pseudocode for Monte Carlo ES is inefficient because, for each state–action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state–action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this.\n",
    "\n",
    "Instead of initializing\n",
    "$$\n",
    "Returns(s,a)\\leftarrow\\text{empty list, for all }s\\in\\mathcal{S},\\ a\\in\\mathcal{A},\n",
    "$$\n",
    "they should be initialized as\n",
    "$$\n",
    "\\begin{align}\n",
    "&Return(s,a)\\leftarrow0\\text{, for all }s\\in\\mathcal{S},\\ a\\in\\mathcal{A} \\\\\n",
    "&N(s,a)\\leftarrow0\\text{, for all }s\\in\\mathcal{S},\\ a\\in\\mathcal{A}\n",
    "\\end{align}\n",
    "$$\n",
    "The altered part of the update rule would then be\n",
    "$$\n",
    "\\begin{align}\n",
    "&N(s,a)\\leftarrow N(s,a)+1 \\\\\n",
    "&Return(s,a)\\leftarrow Return(s,a)+\\frac{1}{N(s,a)}\\left(G-Returns(s,a)\\right)\\\\\n",
    "&Q(S_t,A_t)\\leftarrow Return(s,a).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db722c63-9ee5-44b1-bc75-479a1fa0db0b",
   "metadata": {},
   "source": [
    "## Monte Carlo Control without Exploring Starts\n",
    "\n",
    "* To avoid exploring starts, which is an unrealistic assumption if we do not have direct control over where we start in the environment, we instead ensure visiting all state-action pairs by using $\\varepsilon$-greedy policies\n",
    "* An $\\varepsilon$-greedy policy is a special case of $\\varepsilon$-soft policies, for which each action $a$ is chosen with a probability $\\ge\\frac{\\varepsilon}{|\\mathcal{A}(s)|}$\n",
    "* By performing GPI, we move this policy closer toward the optimal policy\n",
    "* The policy improvement theorem shows this, if we let $\\pi'$ be the policy that is $\\varepsilon$-greed with respect to $v_\\pi$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s,\\pi'(s))&=\\sum_a\\pi'(a|s)q_\\pi(s,a) \\\\\n",
    "&=\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_aq_\\pi(s,a)+(1-\\varepsilon)\\underset{a}{\\operatorname{max}}q_\\pi(s,a) \\\\\n",
    "&\\ge\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_aq_\\pi(s,a)+(1-\\varepsilon)\\sum_a\\frac{\\pi(a|s)-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}}{1-\\varepsilon}q_\\pi(s,a) \\\\\n",
    "&=\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_aq_\\pi(s,a)-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_aq_\\pi(s,a)+\\sum_a\\pi(a|s)q_\\pi(s,a) \\\\\n",
    "&=v_\\pi(s)\n",
    "\\end{align}\n",
    "$$\n",
    "* We define a new environment, in which action $a$ in state $s$ leads to the same state as in the original environment with a probability of $1-\\varepsilon$, and with a probability of $\\frac{\\varepsilon}{|\\mathcal{A}(s)|}$, it chooses a random, new action $a$ instead, and transitions to the state corresponding to that. In this new environment, the optimal value function is $\\tilde{v}_*(s)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{v}_*(s)&=\\underset{a}{\\operatorname{max}}\\sum_{s',r}\\Biggr[(1-\\varepsilon)p(s',r|s,a)+\\sum_{a'}\\frac{\\varepsilon}{|\\mathcal{A}(s)|}p(s',r|s,a')\\Biggr]\\Biggr[r+\\gamma \\tilde{v}_*(s')\\Biggr] \\\\\n",
    "&=(1-\\varepsilon)\\underset{a}{\\operatorname{max}}\\sum_{s',r}p(s',r|s,a)\\Biggr[r+\\gamma \\tilde{v}_*(s')\\Biggr] \\\\\n",
    "&\\quad+\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_{a'}\\sum_{s',r}p(s',r|s,a)\\Biggr[r+\\gamma \\tilde{v}_*(s')\\Biggr] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "* If our estimate has converged, then the equality $q_\\pi(s,\\pi'(s))=v_\\pi(s)$ holds, in which case\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(s)&=\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_aq_\\pi(s,a)+(1-\\varepsilon)\\underset{a}{\\operatorname{max}}q_\\pi(s,a) \\\\\n",
    "&=(1-\\varepsilon)\\underset{a}{\\operatorname{max}}\\sum_{s',r}p(s',r|s,a)\\Biggr[r+v_\\pi(s')\\Biggr] \\\\\n",
    "&\\quad+\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\sum_a\\sum_{s',r}p(s',r|s,a)\\Biggr[r+v_\\pi(s')\\Biggr]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* As this is the same expression as for $\\tilde{v}_*(s)$, which we know to be unique, $\\tilde{v}_*(s)=v_\\pi(s)$\n",
    "* So, policy iteration works for $\\varepsilon$-soft policies in general, and therefore for $\\varepsilon$-greedy policies in particular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477ab85-a111-4aaf-b29b-671ca8ef4e5e",
   "metadata": {},
   "source": [
    "## Off-policy Prediction via Importance Sampling\n",
    "\n",
    "* Learning control methods want to know optimal values from behaving non-optimally\n",
    "* By utilizing $off\\ policy$ methods, this can be done\n",
    "* Keep one $target\\ policy$ $\\pi$, for which to estimate action values, and the behavior policy $b$, with which to generate sample returns\n",
    "* Every action taken under $\\pi$ has to be taken under $b$ as well, in order to estimate the action values\n",
    "* So $b$ must be stochastic in states where it is not equal to $\\pi$\n",
    "* Off-policy methods are slower to converge, due to high variance, but more general and powerful\n",
    "* Sampled trajectories are weighted according to their $importance$-$sampling\\ ratio$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\rho_{t:T-1}&=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} \\\\\n",
    "\\mathbb E_b[\\rho_{t:T-1} G_t|S_t=s]&=v_\\pi(s)\n",
    "\\end{align}\n",
    "$$\n",
    "* To generalize across episodes, increase $t$ across episode boundaries, and let $\\mathcal{T}(s)$ denote the time steps $t$ in which state $s$ was visited, and $T(t)$ the first episode termination after time step $t$, which makes $G_t$ the return starting from time step $t$, ending at $T(t)$\n",
    "* The value estimate of state $s$ is then\n",
    "\n",
    "$$\n",
    "V(s)\\doteq\\frac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{|\\mathcal{T}(s)|}\n",
    "$$\n",
    "* This is $ordinary$ importance sampling, which is unbiased, but has high variance, while $weighted$ importance sampling is biased, but has low variance, often being preferred in practice\n",
    "\n",
    "$$\n",
    "V(s)\\doteq\\frac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}}\n",
    "$$\n",
    "* Deciding between first-visit and every-visit methods, the simpler every-visit method is preferred in practice, and works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea2a3d-afaf-4da7-974b-339f5a94ea8f",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.5}$\n",
    "\n",
    "#### Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability $p$ and transitions to the terminal state with probability $1-p$. Let the reward be $+1$ on all transitions, and let $\\gamma=1$. Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?\n",
    "\n",
    "The first-visit estimate will simply be the return observed from the episode, $10$. The every-visit method, however, will average the returns experienced from all 10 visits to that state, giving $\\frac{\\sum_{i=1}^{10}i}{10}=5.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dafda0-4390-4ad2-a92f-d2365d453537",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.6}$\n",
    "\n",
    "#### What is the equation analogous to (5.6) for $action$ values $Q(s,a)$ instead of state values $V(s)$, again given returns generated using $b$?\n",
    "\n",
    "$$\n",
    "Q(s,a)\\doteq\\frac{\\sum_{t\\in\\mathcal{T}(s,a)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\mathcal{T}(s,a)}\\rho_{t:T(t)-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502a0db-e739-4de7-b482-7cc355ee606b",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.7}$\n",
    "\n",
    "#### In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?\n",
    "\n",
    "I don't know exactly why that would be the case, but my intuition leads me in the direction of high importance-sampling ratios introducing bias early on, as they dominate the estimate, getting averaged out slowly over time. But that does not explain the initially *low* error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d17359-add3-4684-91b1-2b740fa1237d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{5.8}$\n",
    "\n",
    "#### The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not? \n",
    "\n",
    "I'm unable to prove this rigorously, but for every $n$-step episode, the estimate is also updated for the $1, 2, \\dots, n-1$-step sub-episodes contained within, heavily discounting longer episodes in the overall estimate. This should reduce the variance, and at least $feels$ like we have some $\\frac{1}{2^k}$-adjacent series invovled somewhere, which should converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
