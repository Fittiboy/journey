{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffc86eb-9a93-4c55-89e8-e449b6babe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from policy_iteration import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0b377-477f-4112-bba0-d31e99974cfe",
   "metadata": {},
   "source": [
    "# Day 6 - Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f5dc2-dfa6-46bd-b007-e977b98c3da3",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "* $Policy\\ iteration$ is the process of alternating policy evaluation and policy improvement\n",
    "\n",
    "$$\n",
    "\\pi_0\\overset{E}{\\longrightarrow}v_{\\pi_0}\\overset{I}{\\longrightarrow}\\pi_1\\overset{E}{\\longrightarrow}v_{\\pi_1}\\overset{I}{\\longrightarrow}\\pi_2\\overset{E}{\\longrightarrow}\\dots\\overset{I}{\\longrightarrow}\\pi_*\\overset{E}{\\longrightarrow}v_{\\pi_*}\n",
    "$$\n",
    "* This is guaranteed to be a strict improvement at each step, until it reaches the optimal policy, which is guaranteed to happen in a finite number of iterations in a finite MDP\n",
    "* This can often converge in very few iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0966ac36-1681-4050-b4ad-69cea8586469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]\n",
      " [ 0. -2.  0.  0.]]\n",
      "[[0. 3. 3. 2.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 1. 2.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_states = 16\n",
    "num_actions = 4\n",
    "transitions = np.zeros((num_states, num_states, num_actions))\n",
    "for s in range(num_states):\n",
    "    up, right, down, left = s-4, s+1, s+4, s-1\n",
    "    if s == 0:\n",
    "        up, right, down, left = 0, 0, 0, 0\n",
    "    if s in [1, 2, 3]:\n",
    "        up = s\n",
    "    if s in [3, 7, 11]:\n",
    "        right = s\n",
    "    if s in [12, 13, 14]:\n",
    "        down = s\n",
    "    if s in [4, 8, 12]:\n",
    "        left = s\n",
    "    if s == 14:\n",
    "        right = 0\n",
    "    if s == 11:\n",
    "        down = 0\n",
    "    if s == 13:\n",
    "        down = 15\n",
    "    if s == 15:\n",
    "        up = 13\n",
    "        right = 14\n",
    "        down = 15\n",
    "        left = 12\n",
    "    transitions[s,up,0] = 1\n",
    "    transitions[s,right,1] = 1\n",
    "    transitions[s,down,2] = 1\n",
    "    transitions[s,left,3] = 1\n",
    "rewards = np.ones((num_states, num_states, num_actions)) * -1.0\n",
    "rewards[0,:,:] = 0.0\n",
    "discount = 1.0\n",
    "policy = np.ones((num_states, num_actions)) / 4.0 # Order: up, right, down, left\n",
    "\n",
    "iterator = PolicyIteration(transitions, rewards, discount, policy)\n",
    "iterator.find_optimal_policy()\n",
    "\n",
    "values = np.zeros((num_states+4, 1))\n",
    "values[:num_states,:] = iterator.values.copy()\n",
    "values[num_states-1,:] = iterator.values[0]\n",
    "values[17] = iterator.values[15]\n",
    "values = values.reshape(5,4)\n",
    "print(values)\n",
    "policy = np.zeros((num_states+4, 1))\n",
    "policy[:num_states,:] = np.argmax(iterator.policy, axis=2)\n",
    "policy[num_states-1,:] = np.argmax(iterator.policy[0], axis=1)\n",
    "policy[17] = np.argmax(iterator.policy[15], axis=1)\n",
    "policy = policy.reshape(5,4)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f2405-4165-49b3-b26c-7e92b757df33",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.4}$\n",
    "\n",
    "#### The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "The policy may oscillate if the $\\operatorname{argmax}$ breaks ties randomly. To fix this, ties should be broken deterministically, for example by always choosing the first of the maximizing actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f070f7b-9121-4d7e-aab7-d9da291be706",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.5}$\n",
    "\n",
    "#### How would policy iteration be defined for action values? Give a complete algorithm for computing $q_\\pi$, analogous to that on page 80 for computing $v_\\pi$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.\n",
    "\n",
    "1. Initialization <br>\n",
    "   $Q(s,a)\\in\\mathbb R$ and $\\pi(s)\\in \\mathcal{A}(s)$ arbitrarily for all $s\\in\\mathcal{S}, a\\in\\mathcal{A};\\, Q(terminal, a)\\doteq0$ for all $a\\in\\mathcal{A}$\n",
    "\n",
    "2. Policy Evaluation <br>\n",
    "   Loop: <br>\n",
    "   &emsp;$\\Delta\\leftarrow0$<br>\n",
    "   &emsp;Loop for each $s\\in\\mathcal{S},\\ a\\in\\mathcal{A}$:<br>\n",
    "   &emsp;&emsp;$q\\leftarrow Q(s,a)$<br>\n",
    "   &emsp;&emsp;$Q(s,a)\\leftarrow \\sum_{s',r}p(s',r|s,a)[r+\\gamma\\sum_{a'}\\pi(a'|s')Q(s',a')]$<br>\n",
    "   &emsp;&emsp;$\\Delta\\leftarrow\\operatorname{max}(\\Delta,|v-Q(s,a)|)$<br>\n",
    "   until $\\Delta<\\theta$ (a small positive number determining the accuracy of estimation)<br>\n",
    "\n",
    "3. Policy Improvement<br>\n",
    "   $policy$-$stable\\leftarrow true$<br>\n",
    "   For each $s\\in\\mathcal{S}$:<br>\n",
    "   &emsp;$old$-$action\\leftarrow\\pi(s)$<br>\n",
    "   &emsp;$\\pi(s)\\leftarrow\\operatorname{arg}\\underset{a}{\\operatorname{max}}Q(s,a)$<br>\n",
    "   &emsp;$policy$-$stable\\leftarrow old$-$action=\\pi(s)$<br>\n",
    "   If $policy$-$stable$, then stop and return $Q\\approx q_*$ and $\\pi\\approx\\pi_*;$ else go to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db9217-d4a9-4b91-ab19-b2625cd6f872",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.6}$\n",
    "\n",
    "#### Suppose you are restricted to considering only policies that are $\\varepsilon$-soft, meaning that the probability of selecting each action in each state, $s$, is at least $\\varepsilon/|\\mathcal{A}(s)|$. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "\n",
    "In the third step, the values of $\\pi(a|s)$ would have to be set to $\\frac{\\varepsilon}{|\\mathcal{A}(s)|}$ first, then for the best action $a_*$, $\\pi(a_*|s)$ would have to be increased by $1-\\varepsilon$.  \n",
    "In the second step, the new value estimate would have to also sum over all actions, as $\\pi(s)$ is now replaced with $\\pi(a|s)$.  \n",
    "In the first step, $\\pi(a|s)$ would have to be initialized for all $s\\in\\mathcal{S},\\ a\\in\\mathcal{A}$, such that $\\sum_a\\pi(a|s)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1bcba7-3edc-418c-9474-c0863c072dc4",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{4.7}\\ (programming)$\n",
    "\n",
    "#### Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\\\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of \\\\$4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem.\n",
    "\n",
    "TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
