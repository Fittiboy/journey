{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0022885-73ed-4a4f-bec0-869a9ed53202",
   "metadata": {},
   "source": [
    "# Day 4 - Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862bf55-687b-45c8-95fe-4037fc4da59d",
   "metadata": {},
   "source": [
    "## The Agent-Environment Interface\n",
    "\n",
    "* A Markov decision process is defined by an $environment$ and an $agent$\n",
    "* The environment, at each time step $t$, gives the agent the next $state$ and $reward$\n",
    "* The agent then chooses an $action$ to perform, which influences what state the environment transitions to\n",
    "* This formalism is flexible and general:\n",
    "    - Time steps can be arbitrary intervals\n",
    "    - Actions can be low-level or high-level\n",
    "    - States can be concrete sensor readings or abstract, conceptual mental states\n",
    "    - Actions could even be mental/computational"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d21a75-c83f-428e-92f4-937858000c80",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.1}$\n",
    "\n",
    "#### Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "1. Playing Minecraft:\n",
    "    - The states are the frames of the game, or more accurately some learned, lower-dimensional representation of them.\n",
    "    - Actions are keyboard button presses and mouse movements.\n",
    "    - The agent could receive a reward each time it unlocks an Advancement in the game\n",
    "        * As these are very sparse rewards, perhaps rewards could be given every time the agent is surprised in some way, to encourage exploration     \n",
    "1. Cooking pasta:\n",
    "    - The states are information about the level to which the pot is filled with water, the salt content of the water, as well as its temperature\n",
    "    - Actions are filling the pot with water, pouring some water out, adding salt, turning up/down the heat, putting the pasta in the pot, and signalling when the pasta is considered \"done.\"\n",
    "    - The rewards could come in the form of ratings by humans tasting the pasta\n",
    "1. A robot learning to do what a human wants:\n",
    "    - The states would simply be the robot's sensor readings, perhaps compressed into learned representations\n",
    "    - Actions would be voltages applied to the robot's motors\n",
    "    - Rewards come in the form of the human pressing a reward button whenever they're pleased with what the robot did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e881c74-7321-45e0-8858-7111cc122175",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.2}$\n",
    "\n",
    "#### Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "One task that may be difficult to represent as an MDP is to \"make scientific progress.\" It seems quite difficult to define a reward function that somehow represents this goal. Though, if a learning algorithm is sufficiently powerful and sample-efficient, it might be possible to simply use a manual reward signal from a human, given whenever they make the subjective judgment that the agent is moving in the direction of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec379a-0eaf-4ac8-a9df-9e0b64e6880e",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.3}$\n",
    "\n",
    "#### Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "Where to draw this line depends on how well we can control the car, and on how well our agent can actually learn in practice. A vehicle that is already able to drive autonomously, or perhaps simply has a human driver, can still be part of a reinforcement learning problem, where the actions are at the highest level of deciding where to drive. This could be to optimize travel time, for example.  \n",
    "Meanwhile, when the task is to teach a physical robot how to drive a car, and the robot is supposed to also learn other tasks, then, if we have a sufficiently powerful learning algorithm, the lowest level seems like the best choice. Enabling the robot to learn how to move its limbs will give it a more general skill that transfers to different tasks more seamlessly. It is also possible to train a second agent to make the higher-level decisions that guidet he robotic agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413bbc43-a567-4720-a7f5-ecc3326e1a42",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.4}$\n",
    "\n",
    "#### Give a table analogous to that in Example 3.3, but for $p(s',r|s,a)$. It should have columns for $s,\\ a,\\ s',\\ r,\\ $and $p(s',r|s,a)$, and a row for every 4-tuple for which $p(s',r|s,a) > 0$.\n",
    "\n",
    "|$s$ |$a$     |$s'$|$r$         |$p(s',r|s,a)$|\n",
    "|----|--------|----|------------|-------------|\n",
    "|high|search  |high|$r_{search}$|$\\alpha$     |\n",
    "|high|search  |low |$r_{search}$|$1-\\alpha$   |\n",
    "|high|wait    |high|$r_{wait}$  |1            |\n",
    "|low |search  |high|$-3$        |$1-\\beta$    |\n",
    "|low |search  |low |$r_{search}$|$\\beta$      |\n",
    "|low |wait    |low |$r_{wait}$  |1            |\n",
    "|low |recharge|high|$0$         |1            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b0618-2314-445f-92d3-3f27879c08fc",
   "metadata": {},
   "source": [
    "## Goals and Rewards\n",
    "\n",
    "* The $reward\\ hypothesis$ states that all goals can be thought of as the maximization of the expected cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532fa48-dce6-460b-9fd4-9488537c89be",
   "metadata": {},
   "source": [
    "## Returns and Episodes\n",
    "\n",
    "* The return from time step $t$, $G_t$, is defined as the sum of future rewards\n",
    "* For continuing tasks, the $discounted\\ return\\;G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+\\dots=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}$ is used\n",
    "* The goal of the agent is to maximize the expectation of $G_t$\n",
    "* The discouned reward can be written recursively as $G_t=R_{t+1}+\\gamma G_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d9a74-d8a2-4c91-aead-02746faa4c69",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.5}$\n",
    "\n",
    "#### The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "The equation does not take into account that a transition into the terminal state exists, so we need to sum over $s'\\in\\mathcal{S}^+$:\n",
    "$$\n",
    "\\sum_{s'\\in \\mathcal{S}^+}\\sum_{r\\in\\mathcal{R}}p(s',r|s,a)=1,\\ \\text{for all }s\\in\\mathcal{S}, a\\in\\mathcal{A}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed0f4a-4dd7-45f9-902d-2746bb7fffa9",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.6}$\n",
    "\n",
    "#### Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $-1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "The return in this case would simply be $-\\gamma^{K-1}$, where $K$ is again the number of time steps before failure. The difference to the continuing case is that this is the only nonzero reward received, and the return is not a sum of the rewards of all expected future failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7ef33-befc-45f7-af85-fd6f131a848e",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.7}$\n",
    "\n",
    "#### Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "If the reward is always $1$ for escaping, undiscounted, then there is no difference between solving the maze as fast as possible, and taking several decades to solve it. By discounting the reward, or providing a small negative reward on each time step, this problem would be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6708f-ae4a-4239-8828-eb832f54a08d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.8}$\n",
    "\n",
    "#### Suppose  $\\gamma=0.5$ and the following sequence of rewards is received $R_1=-1,\\ R_2=2,\\ R_3=6,\\ R_4=3,\\ \\text{and}\\ R_5=2$, with $T=5$. What are $G_0,G_1,\\dots,G_5$? Hint: Work backwards.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_5&=0 \\\\\n",
    "G_4&=R_5+\\gamma G_5=2 \\\\\n",
    "G_3&=R_4+\\gamma G_4=4 \\\\\n",
    "G_2&=R_3+\\gamma G_3=8 \\\\\n",
    "G_1&=R_2+\\gamma G_2=6 \\\\\n",
    "G_0&=R_1+\\gamma G_1=2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e11e14-7e91-42a6-b1c9-15ea6e5df898",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.9}$\n",
    "\n",
    "#### Suppose $\\gamma=0.9$ and the reward sequence is $R_1=2$ followed by an infinite sequence of $7$s. What are $G_1$ and $G_0$?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_1&=7\\sum_{k=0}^\\infty\\gamma^k=\\frac{7}{1-\\gamma}=70 \\\\\n",
    "G_0&=2+\\gamma G_1=65\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c0261-7d0d-4aeb-8692-df805241999f",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.10}$\n",
    "\n",
    "#### Prove the second equality in (3.10).\n",
    "\n",
    "First, we multiply both sides by $\\gamma$, which gives us\n",
    "$$\n",
    "\\gamma G_t=\\gamma\\sum_{k=0}^\\infty\\gamma^k=\\sum_{k=0}^\\infty\\gamma^{k+1}=\\sum_{k'=1}^\\infty\\gamma^{k'},\n",
    "$$\n",
    "which lets us do\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t-\\gamma G_t&=\\quad\\ \\ \\sum_{k=0}^\\infty\\gamma^{k}-\\sum_{k'=1}^\\infty\\gamma^{k'} \\\\\n",
    "&=1+\\sum_{k=1}^\\infty\\gamma^k-\\sum_{k'=1}^\\infty\\gamma^{k'} \\\\\n",
    "&=1.\n",
    "\\end{align}\n",
    "$$\n",
    "Finally, upon rearranging, we find that\n",
    "$$\n",
    "G_t=\\frac{1}{1-\\gamma}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff962634-6cd5-4fd5-a381-30f151598cd6",
   "metadata": {},
   "source": [
    "## Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "* To unify both cases, we can introduce an $absorbing\\ state$ to turn episodic tasks into continuing tasks\n",
    "* Instead of an episode ending once the terminal state is reached, the absorbing state is reached, which transitions only to itself, giving a reward of 0\n",
    "* Setting $\\gamma=1$ now gives us the same infinite sum for the return as for the continuing case\n",
    "* Alternatively, we can use the unified notation\n",
    "\n",
    "$$\n",
    "G_t\\doteq\\sum_{k=t+1}^T\\gamma^{k-t-1}R_k\n",
    "$$\n",
    "* Here, either $\\gamma=1$, deonting the episodic case, or $T=\\infty$, denoting the continuing case, but never both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c3078-84c4-40a2-9a2b-d325974addae",
   "metadata": {},
   "source": [
    "## Policies and Value Functions\n",
    "\n",
    "* Almost all RL algorithms try to learn $value\\ functions$\n",
    "* The value function of a state is the expected return from that state\n",
    "* A $policy$ is a function mapping from states to the probabilities of taking each action from that state\n",
    "* For all $s\\in\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "v_\\pi(s)\\doteq\\mathbb E_\\pi\\left[G_t|S_t=s\\right]=\\mathbb E_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\Biggr|S_t=s\\right]\n",
    "$$\n",
    "* For $action-values$,\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a)\\doteq\\mathbb E_\\pi\\left[G_t|S_t=s,A_t=a\\right]=\\mathbb E_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\Biggr|S_t=s,A_t=a\\right]\n",
    "$$\n",
    "* These can be estimated from experience\n",
    "* The $Bellman\\ equations$ show the value functions' recursive nature:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(s)&\\doteq\\mathbb E_\\pi[G_t|S_t=s] \\\\\n",
    "&=\\mathbb E_\\pi[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "&=\\sum_a\\pi(a|s)\\sum_{s'}\\sum_rp(s',r|s,a)\\left[r+\\gamma\\mathbb E_\\pi[G_{t+1}|S_{t+1}=s']\\right] \\\\\n",
    "&=\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23934633-858e-42e1-be50-7424bf6d9815",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.11}$\n",
    "\n",
    "#### If the current state is $S_t$, and actions are selected according to a stochastic policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function $p$ (3.2)?\n",
    "\n",
    "$$\n",
    "\\mathbb E[R_{t+1}]=\\sum_a\\pi(a|S_t)\\sum_{s',r}p(s',r|S_t,a)r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e15b5-1a6d-45bf-ab93-3eaae4b2242e",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.12}$\n",
    "\n",
    "#### Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "$$\n",
    "v_\\pi(s)=\\sum_a\\pi(a|s)q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070ac9a-fb9f-4171-becb-12bcba73a997",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.13}$\n",
    "\n",
    "#### Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument $p$.\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a)=\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ba5ba-77f1-4144-b0c6-61ddbc60d7d7",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.14}$\n",
    "\n",
    "#### The Bellman equation (3.14) must hold for each state for the value function $v_\\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at $+0.7$, with respect to its four neighboring states, valued at $+2.3$, $+0.4$, $-0.4$, and $+0.7$. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "$$\n",
    "v_\\pi(center)=0+0.9\\cdot\\frac{2.3-0.4+0.4+0.7}{4}=\\frac{9}{10}\\cdot\\frac{3}{4}\\approx0.7\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524a9ef-fdae-4bc5-8051-1d652e6b984d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.15}$\n",
    "\n",
    "#### In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of $c$ and $\\gamma$?\n",
    "\n",
    "If $G_t'$ is the return received after adding a constant $c$ to each reward, then\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t'&\\doteq(R_{t+1}+c)+\\gamma(R_{t+2}+c)+\\gamma^2(R_{t+3}+c)+\\dots \\\\\n",
    "&=\\sum_{k=0}^\\infty\\gamma^k(R_{t+k+1}+c) \\\\\n",
    "&=\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}+\\sum_{k=0}^\\infty\\gamma^kc \\\\\n",
    "&=G_t+c\\sum_{k=0}^\\infty\\gamma^k \\\\\n",
    "&=G_t+\\frac{c}{1-\\gamma}.\n",
    "\\end{align}\n",
    "$$\n",
    "We can now define the adjusted value to be\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi'(s)&\\doteq\\mathbb E\\left[G_t'|S_t=s\\right] \\\\\n",
    "&=\\mathbb E\\left[G_t+\\frac{c}{1-\\gamma}\\Biggr|S_t=s\\right] \\\\\n",
    "&=\\mathbb E\\left[G_t|S_t=s\\right]+\\frac{c}{1-\\gamma} \\\\\n",
    "&=v_\\pi(s)+\\frac{c}{1-\\gamma}.\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, we can define $$v_c\\doteq\\frac{c}{1-\\gamma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af124ec5-db5b-4578-a477-a49b36c86177",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.16}$\n",
    "\n",
    "#### Now consider adding a constant $c$ to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "In the undiscounted episodic case, adding a constant $c$ to each reward will increase the return of an episode by $cT$. If $c$ is negative, this encourages shorter episode length, while a sufficiently large $c$ can make some nonterminal state transition rewards positive, giving the agent an incentive to get into a loop to acquire infinite reward.  \n",
    "In the maze example, where each reward is $-1$, the agent is usually incentivized to solve the maze as quickly as possible. For any $c>1$, the agent can now accumulate infinite reward by staying in the maze forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702722f4-67fc-4349-92cf-5b1da0da7b9b",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.17}$\n",
    "\n",
    "#### What is the Bellman equation for action values, that is, for $q_\\pi$? It must give the action value $q_\\pi(s,a)$ in terms of the action values, $q_\\pi(s',a')$, of possible successors to the state–action pair $(s,a)$. Hint: The backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s,a)&\\doteq\\mathbb E\\left[G_t|S_t=s,A_t=a\\right] \\\\\n",
    "&=\\mathbb E\\left[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a\\right] \\\\\n",
    "&=\\sum_{s'}\\sum_rp(s',r|s,a)\\left[r+\\gamma\\sum_{a'}\\pi(a'|s')\\mathbb E\\left[G_{t+1}|S_{t+1}=s',A_{t+1}=a'\\right]\\right] \\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3017a34-dbb7-4ce2-8719-b1e487e39115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### $Exercise\\ \\mathcal{3.18}$\n",
    "\n",
    "#### The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    "\n",
    "$$\n",
    "\\text{(See book)}\n",
    "$$\n",
    "#### Give the equation corresponding to this intuition and diagram for the value at the root node, $v_\\pi(s)$, in terms of the value at the expected leaf node, $q_\\pi(s,a)$, given $S_t=s$. This equation should include an expectation conditioned on following the policy, $\\pi$.\n",
    "\n",
    "$$\n",
    "v_\\pi(s)=\\mathbb E_{A_t\\sim\\pi}\\left[q_\\pi(S_t,A_t)|S_t=s\\right]\n",
    "$$\n",
    "\n",
    "#### Then give a second equation in which the expected value is written out explicitly in terms of $\\pi(a|s)$ such that no expected value notation appears in the equation.\n",
    "\n",
    "$$\n",
    "v_\\pi(s)=\\sum_a\\pi(a|s)q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847105b-a8a7-4357-aa09-4a1c33187b71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### $Exercise\\ \\mathcal{3.19}$\n",
    "\n",
    "#### The value of an action, $q_\\pi(s,a)$, depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states:\n",
    "\n",
    "$$\n",
    "\\text{(See book)}\n",
    "$$\n",
    "\n",
    "#### Give the equation corresponding to this intuition and diagram for the action value, $q_\\pi(s,a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value, $v_\\pi(S_{t+1})$, given that $S_t=s$ and $A_t=a$. This equation should include an expectation but not one conditioned on following the policy.\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a)=\\mathbb E_{S_{t+1},R_{t+1}\\sim p}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a\\right]\n",
    "$$\n",
    "\n",
    "#### Then give a second equation, writing out the expected value explicitly in terms of $p(s',r|s,a)$ defined by (3.2), such that no expected value notation appears in the equation.\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a)=\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf764f1-a063-47b3-af6a-6bc33c7069fe",
   "metadata": {},
   "source": [
    "## Optimal Policies and Value Functions\n",
    "\n",
    "* One policy is better than another if the value of *all* states under that policy are larger than the values under the other policy\n",
    "* There is always at least one policy that is better than, or equal to all other policies, called an $optimal\\ policy\\ \\pi_*$\n",
    "* For these policies, there are the optimal state value function $v_*(s)$, and the optimal action value function $q_*(s,a)$\n",
    "* The Bellman equations for these are called the $Bellman\\ optimality\\ equations$\n",
    "* Once $v_*$ is known, any policy that is greedy with respect to a one-step search is an optimal policy\n",
    "* Once $q_*$ is known, an optimal policy simply has to choose the action $a$ that maximizes this\n",
    "* This allows optimal action selection without knowing anything about the environment's dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c7f7f-71dc-496a-8a05-5f6a48ec359c",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.20}$\n",
    "\n",
    "#### Draw or describe the optimal state-value function for the golf example.\n",
    "\n",
    "The optimal state-value function looks like a combination of the state- and action-value functions shown in the book. Anywhere outside of the green, it looks like the optimal action-value function for the driver, as that is the optimal action in those states, while on the green, it looks like the putter state-value function, as that is the optimal action anywhere on the green."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a65f1-81f9-4d90-ad23-22eae976dac7",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.21}$\n",
    "\n",
    "#### Draw or describe the contours of the optimal action-value function for putting, $q_*(s, putter)$, for the golf example.\n",
    "\n",
    "Anywhere on the green, as well as anywhere the green can be reached in one or two strokes by the putter, this looks like the putter state value function in the book. Further out, it looks like the driver action-value function, with each value reduced by $1$, as one stroke is essentially wasted on a putt. This reduction by $1$ does not apply to states where one putt reduces the number of driver strokes required to reach the green, as choosing the driver would not reduce the number of strokes required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c2d33-030a-4354-a69c-40204f56cd81",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.22}$\n",
    "\n",
    "#### Consider the continuing MDP shown to the right. The only decision to be made is that in the top state, where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, $\\pi_{left}$ and $\\pi_{right}$.\n",
    "\n",
    "Naming the top, left, and right states $t$, $l$, and $r$ respectively, we get the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi_{left}}(t)&=1+\\gamma0+\\gamma^21+\\gamma^30+\\dots=\\sum_{k=0}^\\infty(\\gamma^2)^k&&=\\frac{1}{1-\\gamma^2} \\\\\n",
    "v_{\\pi_{right}}(t)&=0+\\gamma2+\\gamma^20+\\gamma^32+\\dots=2\\gamma\\sum_{k=0}^\\infty(\\gamma^2)^k=\\frac{2\\gamma}{1-\\gamma^2}&&=2\\gamma v_{\\pi_{left}}(t) \\\\\n",
    "v_{\\pi_{left}}(l)&=0+\\gamma v_{\\pi_{left}}(t)&&=\\frac{\\gamma}{1-\\gamma^2} \\\\\n",
    "v_{\\pi_{right}}(l)&=0+\\gamma v_{\\pi_{right}}(t)=\\frac{2\\gamma^2}{1-\\gamma^2}&&=2\\gamma v_{\\pi_{left}}(l) \\\\\n",
    "v_{\\pi_{left}}(r)&=2+\\gamma v_{\\pi_{left}}(t)&&=2+\\frac{\\gamma}{1-\\gamma^2} \\\\\n",
    "v_{\\pi_{right}}(r)&=2+\\gamma v_{\\pi_{right}}(t)=2+\\frac{2\\gamma^2}{1-\\gamma^2}&&=2+2\\gamma(v_{\\pi_{left}}(r)-2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### What policy is optimal if $\\gamma=0$?\n",
    "\n",
    "In this case, both policies have the same value in states $l$ and $r$, but $\\pi_{left}$ has a value of $1$ in state $t$, while $\\pi_{right}$ has a value of $0$ there, making $\\pi_{left}$ strictly superior.\n",
    "\n",
    "#### If $\\gamma=0.9$?\n",
    "\n",
    "In each state, as $2\\gamma=1.8>1$, $\\pi_{right}$'s value is superior.\n",
    "\n",
    "#### If $\\gamma=0.5$?\n",
    "\n",
    "Here, $2\\gamma=1$, so the values of both policies are the same in all states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ba7d9-46d0-4625-8c77-1cedd352d942",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.23}$\n",
    "\n",
    "#### Give the Bellman equation for $q_*$ for the recycling robot.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_*(\\mathtt{h},\\mathtt{\\;\\ s})&=\\alpha\\left(r_{search}+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{h},a)\\right)+(1-\\alpha)\\left(r_{search}+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{l},a)\\right) \\\\\n",
    "q_*(\\mathtt{h},\\mathtt{\\;\\ w})&=r_{wait}+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{h},a) \\\\\n",
    "q_*(\\mathtt{l},\\mathtt{\\;\\ s})&=\\beta\\left(r_{search}+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{l},a)\\right)+(1-\\beta)\\left(-3+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{h},a)\\right) \\\\\n",
    "q_*(\\mathtt{l},\\mathtt{\\;\\ w})&=r_{wait}+\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{l},a) \\\\\n",
    "q_*(\\mathtt{l},\\mathtt{re})&=\\gamma\\underset{a'}{\\operatorname{max}}q_*(\\mathtt{h},a)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bdcc99-d0b7-4d29-b948-b9edf881bac0",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.24}$\n",
    "\n",
    "#### Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n",
    "\n",
    "$$\n",
    "v_*(A)=10+\\gamma^510+\\gamma^{10}10+\\dots=\\frac{10}{1-\\gamma^{5}}\\approx24.419\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddbba2-953c-4e03-9417-fd51b501cfe4",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.25}$\n",
    "\n",
    "#### Give an equation for $v_*$ in terms of $q_*$.\n",
    "\n",
    "$$\n",
    "v_*(s)=\\underset{a}{\\operatorname{max}}q_*(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17eb2fc-78e2-4f95-bf24-3adf91260e43",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.26}$\n",
    "\n",
    "#### Give an equation for $q_*$ in terms of $v_*$ and the four-argument $p$.\n",
    "\n",
    "$$\n",
    "q_*(s,a)=\\sum_{s',r}p(s',r|s,a)v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a653eed-1a36-45b2-9d54-7c788a8b0be3",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.27}$\n",
    "\n",
    "#### Give an equation for $\\pi_*$ in terms of $q_*$.\n",
    "\n",
    "$$\n",
    "\\pi_*(s)=\\operatorname{arg}\\underset{a}{\\operatorname{max}}q_*(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d1450-439a-4440-b1af-2f898342c5cf",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.28}$\n",
    "\n",
    "#### Give an equation for $\\pi_*$ in terms of $v_*$ and the four-argument $p$.\n",
    "\n",
    "$$\n",
    "\\pi_*(s)=\\operatorname{arg}\\underset{a}{\\operatorname{max}}\\sum_{s',r}p(s',r|s,a)v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a15860-1e23-4b4b-ad7b-a0e788a06cbf",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{3.29}$\n",
    "\n",
    "#### Rewrite the four Bellman equations for the four value functions ($v_\\pi$, $v_*$, $q_\\pi$, and $q_*$) in terms of the three argument function $p$ (3.4) and the two-argument function $r$ (3.5).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi(s)&=\\sum_a\\pi(a|s)\\sum_{s',r}p(s'|s,a)\\left[r(s,a)+\\gamma v_\\pi(s')\\right] \\\\\n",
    "v_*(s)&=\\underset{a}{\\operatorname{max}}\\sum_{s',r}p(s'|s,a)\\left[r(s,a)+\\gamma v_*(s')\\right] \\\\\n",
    "q_\\pi(s,a)&=\\sum_{s',r}p(s'|s,a)\\left[r(s,a)+\\gamma\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right] \\\\\n",
    "q_*(s,a)&=\\sum_{s',r}p(s'|s,a)\\left[r(s,a)+\\gamma\\underset{a}{\\operatorname{max}}q_*(s',a')\\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfa6c0-4b83-4c2f-8dc4-e213cdbaa383",
   "metadata": {},
   "source": [
    "## Optimality and Approximation\n",
    "\n",
    "* In practice it is almost never possible to find the optimal values and policies\n",
    "* Computation required to calculate all values, and memory required to store them, are not realistic\n",
    "* Using function approximation, memory requirements can be reduced to a small set of parameters\n",
    "* Online learning allows RL algorithms to focus more on states actually encountered, not wasting computation on irrelevant state values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
