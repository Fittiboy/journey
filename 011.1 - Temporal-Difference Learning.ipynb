{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dfa950-bbc3-4dd1-acbc-37f0e0be29f3",
   "metadata": {},
   "source": [
    "# Day 11 - Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de33343-4622-4b99-b972-2b325eb2a899",
   "metadata": {},
   "source": [
    "## Maximization Bias and Double Learning\n",
    "\n",
    "* Imagine a set of actions, all with true values zero, but noisy estimates:\n",
    "    - There is now one action that is seen as maximizing, when in reality, it isn't better than the others\n",
    "    - This is worse if the rewards are, for example, drawn from $\\mathcal{N}(-0.1, 1)$\n",
    "    - There will be positive value estimates, when the true values are actually *worse* than zero\n",
    "* This is called the $maximization\\ bias$\n",
    "* One way to overcome this is to learn two independent estimates, say $Q_1$ and $Q_2$, by alternating which is learned during a given play\n",
    "* The action is chosen according to one estimate, and evaluated according to the other, say $Q_2(\\operatorname{arg}\\underset{a}{\\operatorname{max}}Q_1(a))$\n",
    "* A second, unbiased estimate is the reversal of this, $Q_1(\\operatorname{arg}\\underset{a}{\\operatorname{max}}Q_2(a))$\n",
    "* The update rule for double Q-learning looks like this:\n",
    "\n",
    "$$\n",
    "Q_1(S_t,A_t)\\leftarrow Q_1(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma Q_2\\left(S_{t+1},\\operatorname{arg}\\underset{a}{\\operatorname{max}}Q_1(S_{t+1},a)\\right)-Q_1(S_t,A_t)\\right]\n",
    "$$\n",
    "* This is called $double\\ learning$\n",
    "* The behavior policy can use both estimates, for example by summing or averaging them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3d70f-ed60-4aad-884c-6b8be07f2503",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{6.13}$\n",
    "\n",
    "#### What are the update equations for Double Expected Sarsa with an $\\varepsilon$-greedy target policy?\n",
    "\n",
    "$$\n",
    "Q_1(S_t,A_t)\\leftarrow Q_1(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma \\sum_a\\pi_1(a|S_{t+1})Q_2\\left(S_{t+1},a\\right)-Q_1(S_t,A_t)\\right],\n",
    "$$\n",
    "Where $\\pi_i$ is the policy that is $\\varepsilon$-greedy with respect to the value estimate $Q_i$. The update rule for actions chosen according to $Q_2$ is equivalent, with the indices $1$ and $2$ reversed. The action $A_t$, here, can be determined by some behavior policy $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60b0b8-6b45-4fb2-b908-3fcf44a463a6",
   "metadata": {},
   "source": [
    "## Games, Afterstates, and Other Special Cases\n",
    "\n",
    "* In some environments, like games, the immediate effects of an action are known\n",
    "* After making a move in chess, we know what the $afterstate$ will be\n",
    "* Yet, learning an action-value function means that each state-action pair that leads to this same $afterstate$ will have to learn its value separately\n",
    "* Instead, an afterstate-value function can be learned, which updates the values for all state-action pairs that lead to the same afterstate\n",
    "* There are many different kinds of special problems where small changes like these can improve performance, but the general principles, like GPI still apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c97ad-5a83-4334-a8d3-37d0b9d8d7e1",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{6.14}$\n",
    "\n",
    "#### Describe how the task of Jackâ€™s Car Rental (Example 4.2) could be reformulated in terms of afterstates. Why, in terms of this specific task, would such a reformulation be likely to speed convergence?\n",
    "\n",
    "Suppose there are 10 cars in the first location, and 15 in the second, represented as the state (10, 15). Then, we could move three cars to the first location, placing us in the state (13, 12). The value of this action is the sum of the cost of moving the cars, and the discounted value of the afterstate, (13, 12), from which the day will play out. If we started from the state (13, 12), and moved no cars, the value of this action would be the sum of the immediate cost of $0, and value of that same exact afterstate. Instead of keeping a value estimate for each state-action pair, we could keep a value estimate only for these afterstates, and calculate action-values for action selection using these estimates. As there are many ways to get from several prior states to the same afterstate, this would, first, lower memory requirement, and second, implicitly update the action-values for all state-action pairs landing in some afterstate, once that afterstate's value is updated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
