{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f6ba42-fc03-43f0-8e43-de45e9764847",
   "metadata": {},
   "source": [
    "# Day 28 - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f31d4-cbd4-4aa0-a041-3973f7b03265",
   "metadata": {},
   "source": [
    "Following this [implementation guide](https://chatgpt.com/share/67ac5d36-f610-800e-b057-b16698d8714f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f818951-eea2-4f02-92d2-fb78014ae885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75ce2b6-ecdb-4466-99ab-ccc5196e01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d471108-2234-4b28-adee-4cb5afb84b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x71bcf05c5d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d04fb-cca9-415e-b79d-7803926b95f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Atari Breakout environment with preprocessing wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517e5a2-7c4d-43ce-b162-3a28ce7ec236",
   "metadata": {},
   "source": [
    "We disable frameskip here, but we could also disable it in `AtariPreprocessing`.\n",
    "Otherwise, the frameskips would stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729afb9d-e153-4e74-ae73-f2114525ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=None, frameskip=1)  # no human rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55deed7c-99ac-4ae7-a35c-675b21b35dd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apply Atari-specific preprocessing: grayscale, resize, frame skip, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b2c886-2dd4-4642-8d3d-eff1bf11f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AtariPreprocessing(\n",
    "    env,\n",
    "    screen_size=84,\n",
    "    grayscale_obs=True,\n",
    "    frame_skip=4,\n",
    "    noop_max=30,\n",
    "    scale_obs=True,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03e79a-1ca0-4c0b-a22b-f6007856ad63",
   "metadata": {},
   "source": [
    "- `grayscale_obs`: outputs a single-channel 84x84 image\n",
    "- `frame_skip`: repeat each action for 4 frames (=> 15fps decisions)\n",
    "- `noop_max`: do up to 30 no-op actions at reset (random delay before game starts, common in Atari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b2d23-0c85-4c52-bc94-4c2e30af2636",
   "metadata": {},
   "source": [
    "We also stack last 4 frames to give temporal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4c0c90-594f-463e-870f-8d9f8d2ebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrameStackObservation(env, stack_size=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eeeeda-a0ed-4838-b7e1-5b337dd70fcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Verify environment spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de23f456-3bbf-4361-a2d5-1b023975c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (4, 84, 84)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "obs_shape = env.observation_space.shape  # should be (4, 84, 84) for 4 grayscale frames\n",
    "n_actions = env.action_space.n\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Number of actions:\", n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c2c9e-6b10-4821-8cf0-79f5a5dab934",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57014971-22a1-4792-815c-6c338afbd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500        # number of episodes to train (adjust as needed; Atari usually needs much more)\n",
    "learning_rate = 1e-4      # Adam optimizer learning rate\n",
    "gamma = 0.99              # discount factor for future rewards\n",
    "batch_size = 32\n",
    "buffer_size = 100_000     # replay buffer capacity\n",
    "min_buffer_size = 10_000  # minimum transitions in buffer before training begins\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 1e6       # decay over 1e6 timesteps to epsilon_end\n",
    "target_update_freq = 1000 # how often (steps) to update target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3728d090-a855-4376-a7c3-303c677180ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0f079-4ead-4423-aa0b-1cffbcc3b5d2",
   "metadata": {},
   "source": [
    "## Define the Q-Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff4df69-97ff-4de3-a66c-fb795eefa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input_shape is (C, H, W), e.g., (4, 84, 84)\n",
    "        # The channels, here, are our stacked frames; not colors\n",
    "        c, h, w = input_shape\n",
    "\n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv2d(c, 32, kernel_size=8, stride=4) # Output: 32 x 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)                   # Output: 64 x 9 x 9\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)                   # Output: 64 x 7 x 7\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the conv block\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        # Pass through the fully connected block\n",
    "        x = x.flatten(1) # Flattens all dimensions, starting from 1\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4fc8b-8d4b-497d-8a92-54d1e7a500e4",
   "metadata": {},
   "source": [
    "We now initialize both the policy network, as well as the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4b7083-2550-4926-a403-6165b234d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(obs_shape, n_actions).to(device)\n",
    "target_net = DQN(obs_shape, n_actions).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ded55a-423a-49db-8a74-e576a5181216",
   "metadata": {},
   "source": [
    "We then copy the weights over from the policy network, and put the target network into eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef1121a-18bf-46f6-8af9-17fc4af680fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # Avoids computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24eb74c-20b5-4252-8525-b2af3e7b69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has a total of 1,686,180 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The network has a total of\",\n",
    "    f\"{sum(p.numel() for p in policy_net.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e26f4b-4154-4a4a-b9a6-67dfca2510a5",
   "metadata": {},
   "source": [
    "## Implementing the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03406d33-63b8-4d44-a1ff-d85123128e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97340776-e52b-48e3-8c83-3b8b2b032e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the buffer\"\"\"\n",
    "        # Pixels range from 0 to 255, so they are u8\n",
    "        # Gym may return lazy frame objects, which we .copy() to\n",
    "        # ensure that we have actual pixel data\n",
    "        state = state.copy()\n",
    "        next_state = next_state.copy()\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        # Neat trick for unpacking the list\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states_arr = np.array(states, copy=False) # (4, 84, 84)\n",
    "        next_states_arr = np.array(next_states, copy=False)\n",
    "        actions_arr = np.array(actions, dtype=np.int64)\n",
    "        rewards_arr = np.array(rewards, dtype=np.float32)\n",
    "        # dones are stored as floats, so that we can use the value\n",
    "        # to zero out next_state values when an episode is over\n",
    "        dones_arr = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        states_t = torch.tensor(states_arr, device=device)\n",
    "        next_states_t = torch.tensor(next_states_arr, device=device)\n",
    "        actions_t = torch.tensor(actions_arr, device=device)\n",
    "        rewards_t = torch.tensor(rewards_arr, device=device)\n",
    "        dones_t = torch.tensor(dones_arr, device=device)\n",
    "\n",
    "        return states_t, actions_t, rewards_t, next_states_t, dones_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9c6cc-a37a-40b1-b1b4-66068464ab2b",
   "metadata": {},
   "source": [
    "We then immediately initialize the buffer we will use during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b648eb-97d5-407c-8e00-0796e128575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size, obs_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b864b6-9271-4741-acab-40530103d99b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdd3f134-fd21-4a86-9a60-69117d716875",
   "metadata": {},
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss() # This is the Huber loss\n",
    "\n",
    "epsilon = epsilon_start\n",
    "epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay\n",
    "\n",
    "global_step = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    state = np.array(state, copy=False)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_t = torch.tensor(state, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_t)\n",
    "                action = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon -= epsilon_decay_rate\n",
    "\n",
    "        # Environment step\n",
    "        next_state, reward, done, *_ = env.step(action)\n",
    "        next_state = np.array(next_state, copy=False)\n",
    "\n",
    "        # Store transition\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update values\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        global_step += 1\n",
    "\n",
    "        # Learn from replay\n",
    "        if len(replay_buffer) >= min_buffer_size:\n",
    "            # Sample a batch\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Compute q values for all states and actions\n",
    "            q_values = policy_net(states_b) # (batch_size, n_actions)\n",
    "\n",
    "            # Gather the q values for the actions taken in the batch\n",
    "            # This will result in a tensor like:\n",
    "            # [q_0, q_2, q_1, ...],\n",
    "            # if the actions taken were:\n",
    "            # [a_0, a_2, a_1, ...]\n",
    "            state_action_values = q_values.gather(1, actions_b.view(-1, 1)).squeeze(1)\n",
    "\n",
    "            # Compute the targets\n",
    "            with torch.no_grad():\n",
    "                # rewards_b + gamma * max(target_net(next_states_b), dim=1) - state_action_values\n",
    "                next_q_values = target_net(next_states_b)\n",
    "                max_next_q_values, _ = next_q_values.max(dim=1) # torch.max also returns indices\n",
    "\n",
    "                # Only include the next state if we are not done\n",
    "                targets = rewards_b + gamma * max_next_q_values * (1.0 - dones_b)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(state_action_values, targets)\n",
    "\n",
    "            # Optimize the policy_net parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the target_net periodically\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Track the return, now that the episode is over\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Print the current results\n",
    "        if (episode+1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(\n",
    "                f\"Episode {episode+1}: Reward: {total_reward}, Avg (last 10): {avg_reward:.2f}\",\n",
    "                f\"Epsilon: {epsilon:.3f}\",\n",
    "                end=\"\\t\\t\\r\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06ede7-065f-4970-992b-98b3facd31ae",
   "metadata": {},
   "source": [
    "## Tracking and Debugging with Weights & Biases (W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055e146a-b174-4000-a1bf-2d06e4934fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fitti/.conda/envs/journey/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/fitti/journey/videos/dqn_breakout_tutorial folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos/dqn_breakout_tutorial\"\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\", frameskip=1)\n",
    "env = RecordVideo(\n",
    "    env=env,\n",
    "    video_folder=video_folder,\n",
    "    episode_trigger=lambda x: x % 10 == 0,\n",
    ")\n",
    "\n",
    "env = AtariPreprocessing(\n",
    "    env,\n",
    "    screen_size=84,\n",
    "    grayscale_obs=True,\n",
    "    frame_skip=4,\n",
    "    noop_max=30,\n",
    "    scale_obs=True,\n",
    ")  \n",
    "\n",
    "env = FrameStackObservation(env, stack_size=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff66a18b-3c29-4d61-bc15-0f90ed0e92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.observation_space.shape  # should be (4, 84, 84) for 4 grayscale frames\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1263fdf9-6c2e-4dcf-9b32-a896fae710e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10_000     # number of episodes to train (adjust as needed; Atari usually needs much more)\n",
    "learning_rate = 1e-4      # Adam optimizer learning rate\n",
    "gamma = 0.99              # discount factor for future rewards\n",
    "batch_size = 64\n",
    "buffer_size = 100_000     # replay buffer capacity\n",
    "min_buffer_size = 10_000  # minimum transitions in buffer before training begins\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 1e6       # decay over 1e6 timesteps to epsilon_end\n",
    "target_update_freq = 1000 # how often (steps) to update target network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33532d-c431-43c2-b67e-21fe55593ef4",
   "metadata": {},
   "source": [
    "### Start a new W&B run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7ba007-e627-4f20-9cc8-2cc5f535cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b3820d-5117-46f1-8f64-44962ae29bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfitti\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/journey/wandb/run-20250212_114224-zawjj5b1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/dqn-breakout-tutorial/runs/zawjj5b1' target=\"_blank\">morning-brook-7</a></strong> to <a href='https://wandb.ai/fitti/dqn-breakout-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/dqn-breakout-tutorial' target=\"_blank\">https://wandb.ai/fitti/dqn-breakout-tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/dqn-breakout-tutorial/runs/zawjj5b1' target=\"_blank\">https://wandb.ai/fitti/dqn-breakout-tutorial/runs/zawjj5b1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fitti/dqn-breakout-tutorial/runs/zawjj5b1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71bbe946c470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"dqn-breakout-tutorial\",\n",
    "    config={ # Hyperparameters and config\n",
    "        \"env\": \"ALE/Breakout-v5\",\n",
    "        \"episodes\": num_episodes,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"min_buffer_size\": min_buffer_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"epsilon_start\": epsilon_start,\n",
    "        \"epsilon_end\": epsilon_end,\n",
    "        \"epsilon_decay_steps\": epsilon_decay,\n",
    "        \"target_update_freq\": target_update_freq,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083f676-47af-4f80-94b7-575a44c9f061",
   "metadata": {},
   "source": [
    "### Run the updated training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b8fc683-0afc-44d4-9f62-cded9bcd82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(obs_shape, n_actions).to(device)\n",
    "target_net = DQN(obs_shape, n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # Avoids computing gradients\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size, obs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94833293-b18d-4bcf-8d66-b5ca112a9d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705494b6aa53480b98c86f1f554db866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180: Reward: 1.0, Avg (last 10): 1.00 Epsilon: 0.971\t\t"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "epsilon = epsilon_start\n",
    "epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay\n",
    "\n",
    "global_step = 0\n",
    "episode_rewards = []\n",
    "running_loss = 0.0\n",
    "loss_count = 0\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    state = np.array(state, copy=False)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_t = torch.tensor(state, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_t)\n",
    "                action = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon -= epsilon_decay_rate\n",
    "\n",
    "        # Environment step\n",
    "        next_state, reward, done, *_ = env.step(action)\n",
    "        next_state = np.array(next_state, copy=False)\n",
    "\n",
    "        # Store transition\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update values\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        global_step += 1\n",
    "\n",
    "        # Learn from replay\n",
    "        if len(replay_buffer) >= min_buffer_size:\n",
    "            # Sample a batch\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Compute q values for all states and actions\n",
    "            q_values = policy_net(states_b) # (batch_size, n_actions)\n",
    "\n",
    "            # Gather the q values for the actions taken in the batch\n",
    "            # This will result in a tensor like:\n",
    "            # [q_0, q_2, q_1, ...],\n",
    "            # if the actions taken were:\n",
    "            # [a_0, a_2, a_1, ...]\n",
    "            state_action_values = q_values.gather(1, actions_b.view(-1, 1)).squeeze(1)\n",
    "\n",
    "            # Compute the targets\n",
    "            with torch.no_grad():\n",
    "                # rewards_b + gamma * max(target_net(next_states_b), dim=1) - state_action_values\n",
    "                next_q_values = target_net(next_states_b)\n",
    "                max_next_q_values, _ = next_q_values.max(dim=1) # torch.max also returns indices\n",
    "\n",
    "                # Only include the next state if we are not done\n",
    "                targets = rewards_b + gamma * max_next_q_values * (1.0 - dones_b)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(state_action_values, targets)\n",
    "\n",
    "            # Optimize the policy_net parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the target_net periodically\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            # Accumulate loss for logging\n",
    "            running_loss += loss.item()\n",
    "            loss_count += 1\n",
    "\n",
    "        # Track the return, now that the episode is over\n",
    "        episode_rewards.append(total_reward)\n",
    "        avg_reward_100 = np.mean(episode_rewards[-100:])\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        if loss_count > 0:\n",
    "            avg_loss = running_loss / loss_count\n",
    "        else:\n",
    "            avg_loss = None\n",
    "            \n",
    "        wandb.log({\n",
    "            \"episode\": episode,\n",
    "            \"episode_reward\": total_reward,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"avg_reward_100\": avg_reward_100,\n",
    "            \"avg_loss\": avg_loss,\n",
    "        })\n",
    "\n",
    "        # Reset running loss counters\n",
    "        running_loss = 0.0\n",
    "        loss_count = 0\n",
    "\n",
    "        # Periodically show results and log videos\n",
    "        if (episode+1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(\n",
    "                f\"Episode {episode+1}: Reward: {total_reward}, Avg (last 10): {avg_reward:.2f}\",\n",
    "                f\"Epsilon: {epsilon:.3f}\",\n",
    "                end=\"\\t\\t\\r\",\n",
    "            )\n",
    "\n",
    "            # Log the most recent video that was recorded\n",
    "            latest_video = max(Path(video_folder).glob('*'), key=os.path.getctime)\n",
    "            wandb.log({\n",
    "                \"video\": wandb.Video(str(latest_video))\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6977e-56d6-4e3e-a250-f0e2d7eea18f",
   "metadata": {},
   "source": [
    "## Evaluation: Testing the Learned Agent and Observing Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b8ddc-3650-4022-b6fd-3a6308ce70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.eval()\n",
    "\n",
    "num_test_episodes = 5\n",
    "test_rewards = []\n",
    "\n",
    "for i in range(num_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state, copy=False)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, device=device).unsqueeze(0)\n",
    "        q_values = policy_net(state_t)\n",
    "        action = int(torch.argmax(q_values, dim=1).item())\n",
    "        next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
