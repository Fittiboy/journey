{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c0c7236-39e3-4b0b-a42d-b4beb60a4bd6",
   "metadata": {},
   "source": [
    "# Day 1 â€” Introduction\n",
    "\n",
    "Learning happens by interacting with our environment.\n",
    "Whatever it is we're learning, we notice how our environment reacts to our actions, and we seek to influence what happens.\n",
    "\n",
    "The book ([Sutton & Barto](http://incompleteideas.net/book/RLbook2020.pdf)) explores the computational approach to the above. Looking at idealized situations, we try to understand the best way to learn and act, exploring different designs and methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "177cf04a-963f-4b6b-bdf0-13b61917f0d3",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "* Learn what to do to maximize reward\n",
    "* Learner must discover actions and rewards itself\n",
    "* All actions may affect far future rewards\n",
    "* Distinguishing features of RL: trial-and-error search + delayed reward\n",
    "* RL is a class of problems, the solutions to these problems, and the entire field studying these\n",
    "* Ideas from dynamical systems theory: optimal control of incompleteley known Markov decision processes\n",
    "* Agent must sense the environment, be able to act in it, and have goals to achieve in it\n",
    "    * Sensation, Action, Goal\n",
    "* Agents have to learn from experience; supervised learning is not feasible, due to lack of labeled data\n",
    "* Unlike unsupervised learning, RL doesn't aim to find structure, but to maximize reward\n",
    "* One challenge: Exploration vs. Exploitation\n",
    "    * Learning more vs. Using knowledge to gain reward\n",
    "* RL considers the whole problem of goal-directed acting, not focusing on subproblems in isolation\n",
    "* Subproblems are considered only in service of a complete, goal-seeking agent\n",
    "* A \"complete\" agent can be part of a larger agent\n",
    "* RL is part of a trend towards greater interdisciplinary integration\n",
    "* Many core RL algorithms are inspired by how real biological systems learn\n",
    "* Modern AI and RL focus more on the search for general principles, as opposed to a collection of many specialized methods for intelligence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80b182da-cea6-45dd-8fb8-d26a8ccb4743",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "* Agents need to asses past experience, immediate concerns, and plans for the future, while keeping track of their internal state and the external environment, and setting and pursuing goals\n",
    "* Each action can have indirect, future consequences that need to be taken into account for later planning\n",
    "* All action involves goals, the progress towards which can be judged by the agent directly, to guide behavior\n",
    "* The agent continuously learns from its experience to improve its behavior in the future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59990cd6-079f-4b45-96eb-f6a06f53f62e",
   "metadata": {},
   "source": [
    "## Elements of Reinforcement Learning\n",
    "\n",
    "* Four main elements:\n",
    "    1. Policy\n",
    "    2. Reward signal\n",
    "    3. Value function\n",
    "    4. Environment model\n",
    "* Policy maps from perceived states to actions to perform\n",
    "    - A form of stimulus-response\n",
    "    - Can be a simple lookup table, or involve complex planning and search\n",
    "* Reward signal defines the goal of a problem\n",
    "    - A single number given by the environment at each time step\n",
    "    - Goal is to maximize this over time\n",
    "    - Defines what's good and bad\n",
    "    - Like pleasure and pain\n",
    "    - Primary basis for altering the policy\n",
    "* Value function specifies long-term reward\n",
    "    - Value of a state is expected accumulated future reward\n",
    "    - Rewards: immediate, intrinsic desirability; Values: long-term desirability\n",
    "    - Low reward might be followed by very high reward, making value an important concept\n",
    "    - Rewards: Pleasure and pain; Values: Higher level judgment of desirability of environment\n",
    "    - Without rewards, no values; Purpose of values is to maximize rewards\n",
    "    - All choices are made based on values\n",
    "    - Values are much harder to determine than immediately received rewards\n",
    "    - **Most important component of RL: Efficient method for estimating values**\n",
    "* Environment model\n",
    "    - Predicts next state and reward, based on current state and chosen action\n",
    "    - Used for planning, considering future situations\n",
    "    - Model-based methods exist in contrast to simpler, model-free methods\n",
    "    - Model-based: Long term planning; Model-free: Simple trial-and-error learning (~opposite of planning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a55ee994-8e3e-4020-a75f-5c3cd6173525",
   "metadata": {},
   "source": [
    "## Limitations and Scope\n",
    "\n",
    "* State is to be thought of as what the agents knows about the environment\n",
    "* Issue of constructing this state representation not considered in the book, to focus on decision-making\n",
    "* Most methods considered are concerned with estimating value functions, but not strictly necessary\n",
    "    - Alternatives:\n",
    "        1. Genetic algorithms\n",
    "        2. Genetic programming\n",
    "        3. Simulated annealing\n",
    "        4. Other optimization methods\n",
    "    - Applying multiple static policies, selecting the best (+ random variations) for next generation\n",
    "    - These are called evolutionay methods\n",
    "* In small, or well-structured policy space, evolutionary methods can find good policies\n",
    "* Advantageous if the state cannot be fully sensed\n",
    "* Our focus is on learning by interacting with the environment\n",
    "* Evolutionary methods ignore important structure of the problem\n",
    "    - The fact that policies map from states to actions\n",
    "    - Information about what states are actually visited\n",
    "    - Which actions agents select in their lifetimes\n",
    "* Evolutionary methods and learning share features and may work well together\n",
    "* Evolutionary methods not considered useful *on their own*, for RL problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400c32c-9b8d-4c99-b63a-8aa7cf57eb44",
   "metadata": {},
   "source": [
    "## An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "| | | |\n",
    "|-|-|-|\n",
    "|X|O|O|\n",
    "|O|X|X|\n",
    "| | |X|\n",
    "\n",
    "* Classical optimization methods can compute optimal solution, but:\n",
    "* They require a complete specification of the opponent for this\n",
    "* This information is not available a priori, in most interesting problems\n",
    "* The information can be estimated from experience\n",
    "* First, learn a model of the opponent's behavior\n",
    "* Then, apply dynamic programming to compute optimal solution\n",
    "* Similar to reinforcement learning methods\n",
    "* Evolutionary method would directly search policy space\n",
    "* It would hill-climb in policy space, achieving incremental improvement\n",
    "* Hundreds of different optimization methods could be applied for this\n",
    "* With a value function, we would try to estimate each state's probability of leading to a win\n",
    "    - State $\\mathsf A$ is \"better than\" state $\\mathsf B$ if it has a higher value; a higher probability of leading to a win\n",
    "* We then play many games against the opponent\n",
    "* To choose an action, we examine the value of the resulting state of each action\n",
    "* Most of the time, choose *greedily*, sometimes choose at random to explore\n",
    "* While playing, we adjust the estimated values\n",
    "* After each greedy move, we update the previous state's value to be closer to the next state's value\n",
    "* If $S_t$ is the state before the move, and $S_{t+1}$ the state after the move, the update to the estimated value of $S_t$, $V(S_t)$, looks like this: $$V(S_t)\\leftarrow V(S_t)+\\alpha\\left[V(S_{t+1})-V(S_t)\\right] $$\n",
    "    - $\\alpha$: Small positive fraction, *step-size parameter*; influences rate of learning\n",
    "* Update rule is an example of *temporal-difference learning*; changes based on difference between estimates at two successive time steps\n",
    "* If $\\alpha$ is reduced to zero properly over time, estimates approach true winning probabilities against static opponent\n",
    "* If $\\alpha$ is not reduced to zero, the agent can even adapt to a changing opponent\n",
    "* Evolutionary methods hold a policy fixed while evaluating over many games\n",
    "* Value-based methods use information gained *during each game* to update estimates\n",
    "* Evolutionary methods only look at the final outcome, favoring policies with moves that were never even seen\n",
    "* RL emphasizes learning while interacting\n",
    "* Thre is a clear goal, and correct behavior requires planning\n",
    "* Multi-move traps can be set even without explicit lookahead, without a model\n",
    "* RL applies even without an adversary, in \"games against nature\"\n",
    "* Also applicable when behavior continues indefinitely, with rewards of different magnitudes arriving at any given time\n",
    "* Also applicable when there are no discrete time steps, though theory gets more complicated\n",
    "* Can even be used with large, or infinite state spaces, like the $~10^{20}$ states in backgammon, using neural networks\n",
    "    - See Gerry Tesauro's TD-Gammon\n",
    "* Neural networks allow generalization across similar states from experience\n",
    "* How well an RL system works is closely tied to its ability to generalize\n",
    "* This area shows the greates need for supervised learning\n",
    "* ANNs and Deep Learning are great, but not the only applicable methods for this\n",
    "* While the Tic-Tac-Toe agent had no prior knowledge, RL allows incorporating such knowledge\n",
    "* While the true state was known in Tic-Tac-Toe, RL can also be applied when some information is hidden\n",
    "* In many situations, there is no known model of the environment at all, but RL still works\n",
    "* Model-free methods cannot even think about how their actions will change the environment\n",
    "* They have an advantage when constructing a model is a bottleneck\n",
    "* Model-free methods are also building blocks for model-based methods\n",
    "* RL can be used at low, single-move levels, while also being applied to higher levels, where each \"action\" is the application of some elaborate problem-solving method\n",
    "* RL can operate at multiple of these levels at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff86aa6-8f07-4b45-9298-7d7d1e07d105",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37411440-a8b6-4046-8ed3-cbb1e832d609",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{1.1}$*:*$\\ Self$-$Play$\n",
    "\n",
    "#### Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n",
    "\n",
    "It would have to learn a different policy over time, as any moves that were good in the past, were good against a past version of itself. A move that led to a win against the original agent may no longer work at all after the policy is adjusted the first time. Applied for long enough, this method would probably find the optimal strategy for Tic-Tac-Toe, leading to a draw every time, unless exploratory actions are taken."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce281004-c796-43fa-b2d5-0c15afaf41ad",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{1.2}$*:*$\\ Symmetries$\n",
    "\n",
    "#### Many tic-tac-toe positions appear diferent but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this?\n",
    "\n",
    "To make use of symmetries, states that are equal according to these should share the same value, and be updated simultaneously every time one of them is encountered.\n",
    "\n",
    "#### In what ways would this change improve the learning process?\n",
    "\n",
    "The learning process would be sped up, as this introduces generalization across states, where learning from one state changes the values of all the equivalent states.\n",
    "\n",
    "#### Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we?\n",
    "\n",
    "If the opponent does not take advantage of the states' symmetries, then it might act differently in two states that we consider to be the same state. In this case, taking the symmetries into account oversimplifies the problem by throwing away information about how our opponent acts. Instead of seeing that the opponent takes deterministic moves in two distinct states, we may infer that the opponent randomly picks one of two actions in the \"same\" state.\n",
    "\n",
    "#### Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "In that case, symmetrically equivalent positions should not share values. It might be best to let the learning algorithm decide how to generalize across states, whenever such generalization is actually beneficial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae3ac11-ef00-43c4-9d96-3fdcf4c40c62",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{1.3}$*:*$\\ Greedy\\ Play$\n",
    "\n",
    "#### Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?\n",
    "\n",
    "The agent would quickly learn to follow one specific path from each state, without considering that its estimates of some states might be pessimistic. It may never visit a state it thinks to be of low value, which in reality might lead to another state that has a much higher value. It may outperform a nongreedy player in early games, but it might not be able to adapt as fast as the nongreedy player."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47dff120-4841-4ff8-8d59-877d25b07c95",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{1.4}$*:*$\\ Learning\\ from\\ Exploration$\n",
    "\n",
    "#### Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?\n",
    "\n",
    "1. When we do learn from exploratory moves, the probabilities learned are the true winning probabilities of the agent that continues exploring at random, just as it has during learning.\n",
    "2. When we do *not* learn from exploratory moves, then the values converge to those that assume that exploratory actions are not taken.\n",
    "\n",
    "In the first case, a state from which the agent can immediately win will still not have a value of $1$, as there is a small chance the agent chooses a random move instead of the winning move. In the second case, the value of such a state would approach exactly $1$, as the chance of winning, given that a greedy move is taken, is $100\\%$.\n",
    "\n",
    "#### Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "Under this assumption, learning the values of only greedy moves may lead to problems, especially if there are states where the greedy move leads to a very high chance of winning, while any other move leads to an immediate loss. If we assume that the greedy action is alwasy taken, such a state would have a very high value, but will regularly lead to losses, whenever a random, nongreedy action is taken instead.  \n",
    "If the values are updated on exploratory steps as well, then the agent will take this random chance of an immediate loss into account when choosing the next action, and might avoid such a precarious situation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d70f48de-a009-48f2-bdf3-343ffd8f71bd",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{1.5}$*:*$\\ Other\\ Improvements$\n",
    "\n",
    "#### Can you think of other ways to improve the reinforcement learning player?\n",
    "\n",
    "Instead of only considering the value of the immediate next states, the agent could look ahead several moves into the future for each value update.\n",
    "Additionally, instead of choosing exploratory actions at random, there could be some heuristic for choosing actions that result in the highest information gain.\n",
    "\n",
    "#### Can you think of any better way to solve the tic-tac-toe problem as posed?\n",
    "\n",
    "As posed, I cannot think of any method that is clearly better than what was described."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1e8226-0d33-4da4-9c96-e6689bccc3dd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* RL is an approach to understanding and automating goal-directed learning and decision-making\n",
    "* Emphasis is on learning from interaction with an environment, without requiring supervision or complete models\n",
    "* The first field to address the computational issues that arise when learning from interaction to achieve long-term goals\n",
    "* RL uses the framework of Markov decision processes:\n",
    "    - States\n",
    "    - Actions\n",
    "    - Rewards\n",
    "* MDPs are a simple way of representing essential features of the AI problem, including:\n",
    "    - Cause and effect\n",
    "    - Uncertainty and nondeterminism\n",
    "    - Explicit goals\n",
    "* The concepts of values and value functions are key to most RL methods, are important for efficient search of policies\n",
    "* Value functions distinguish RL from evolutionary methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "679393ad-0de1-459e-a815-25c021ec2b83",
   "metadata": {},
   "source": [
    "## Early History of Reinforcement Learning\n",
    "\n",
    "* Check out Minsky's \"[Steps](https://courses.csail.mit.edu/6.803/pdf/steps.pdf)\" paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
