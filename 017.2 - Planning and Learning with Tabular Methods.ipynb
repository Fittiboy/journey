{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f1eecd-571b-490e-90df-9b40431f1d49",
   "metadata": {},
   "source": [
    "# Day 17 - Planning and Learning with Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae686d3c-c3bb-4b78-a0ef-14318ed83850",
   "metadata": {},
   "source": [
    "## When the Model Is Wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f95462-dbe3-4e4a-bd69-69f695c4500e",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{8.2}$\n",
    "\n",
    "#### Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?\n",
    "\n",
    "Once both agents found the path to the goal, the default Dyna-Q agent would have to keep repeating suboptimal actions until the value estimate of a \"neighboring,\" optimal action is greater. The Dyna-Q+ agent explores these sooner, and therefore finds an ~optimal solution quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff6b2f-4ff9-492f-bf4d-f6ff7a92054d",
   "metadata": {},
   "source": [
    "### $Exercise\\ \\mathcal{8.3}$\n",
    "\n",
    "#### Careful inspection of Figure 8.5 reveals that the diâ†µerence between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?\n",
    "\n",
    "As the Dyna-Q+ agent keeps exploring, it can never reach the optimal policy, which Dyna-Q does. So while Dyna-Q+ finds better solutions faster, Dyna-Q finds the optimal policy at some point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
