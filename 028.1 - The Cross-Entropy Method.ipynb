{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d26213d-b5bf-4254-b97a-9d5dfa734e30",
   "metadata": {},
   "source": [
    "# Day 28 - The Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc0285-99f7-4419-aac1-75a5b8d0f2e8",
   "metadata": {},
   "source": [
    "* The cross-entropy method is model-free, policy-based, and on-policy\n",
    "* The method itself is quite simple\n",
    "    1. Play $N$ episodes\n",
    "    2. Calculate returns and set a boundaryâ€”usually 50th or 70th percentile\n",
    "    3. Discard episodes below the boundary\n",
    "    4. Perform supervised learning of the policy, using the remaining episodes\n",
    "    5. Be satisfied, or goto 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e705ce6-36d8-4fa2-9e2a-544323989a22",
   "metadata": {},
   "source": [
    "## The cross-entropy method on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e956a568-88c7-40f9-b064-dd7eedce5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from dataclasses import dataclass\n",
    "import typing as tt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d67c1af-2c44-4eaf-b352-c48bea9cf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # This is extremely slow on the GPU, due to all the data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf39d9e-6bb9-4855-a086-6d9361d9abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e848f4b0-478b-47aa-819e-6e4cb7eca82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStep:\n",
    "    observation: np.ndarray\n",
    "    action: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    reward: float\n",
    "    steps: tt.List[EpisodeStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179351b1-958e-4adf-8e6f-b3701e3abe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env: gym.Env, net: Net, batch_size: int) -> tt.Generator[tt.List[Episode], None, None]:\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs, _ = env.reset()\n",
    "    sm = nn.Softmax(dim=1).to(device=device)\n",
    "\n",
    "    while True:\n",
    "        obs_v = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        act_probs_v = sm(net(obs_v.unsqueeze(0)))\n",
    "        act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, is_trunc, _ = env.step(action)\n",
    "        episode_reward += float(reward)\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done or is_trunc:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs, _ = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93cfdf2-cc12-48fa-9024-61e0185ace87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: tt.List[Episode], percentile: float) -> tt.Tuple[torch.FloatTensor, torch.LongTensor, float, float]:\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = float(np.percentile(rewards, percentile))\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs: tt.List[np.ndarray] = []\n",
    "    train_act: tt.List[int] = []\n",
    "\n",
    "    for episode in batch:\n",
    "        if episode.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, episode.steps))\n",
    "        train_act.extend(map(lambda step: step.action, episode.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(np.vstack(train_obs))\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90da19e7-9fb3-44a1-99c8-d928271fdfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fitti/.conda/envs/journey/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/fitti/journey/DRL/videos/cartpole-CEM folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "video_folder = \"./DRL/videos/cartpole-CEM\"\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=video_folder)\n",
    "assert env.observation_space.shape is not None\n",
    "obs_size = env.observation_space.shape[0]\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "n_actions = int(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a880263-ac2c-4b92-94e2-4037f2c42812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "36: loss=0.106, reward_mean=500.0, rw_bound=500.0\t\t\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "net = Net(obs_size, HIDDEN_SIZE, n_actions).to(device=device)\n",
    "print(net)\n",
    "objective = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=1e-1)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v.to(device=device, non_blocking=True))\n",
    "    loss_v = objective(action_scores_v, acts_v.to(device=device))\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\n",
    "        \"{0}: loss={1:.3f}, reward_mean={2:.1f}, rw_bound={3:.1f}\".format(\n",
    "        iter_no, loss_v.item(), reward_m, reward_b),\n",
    "        end=\"\\t\\t\\r\"\n",
    "    )\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 499.9:\n",
    "        print(\"\\nSolved!\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf6c7fa-1936-4134-b588-a579409cd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rl-video-episode-1000.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./DRL/videos/cartpole-CEM/rl-video-episode-1000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of video files with their modification times\n",
    "video_files = [(f, os.path.getmtime(os.path.join(video_folder, f))) \n",
    "               for f in os.listdir(video_folder)]\n",
    "\n",
    "# Sort by modification time (newest first) and get the most recent file\n",
    "most_recent_video = sorted(video_files, key=lambda x: x[1], reverse=True)[0][0]\n",
    "print(most_recent_video)\n",
    "\n",
    "# Display the most recent video\n",
    "Video(url=video_folder + \"/\" + most_recent_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
