{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac974d59-2a7c-4d54-b7ec-39633bed604b",
   "metadata": {},
   "source": [
    "# Day 29 - REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215614b-ed16-4f78-8c08-3c07adf1f536",
   "metadata": {},
   "source": [
    "## Implementation: REINFORCE in Atari Breakout (Gymnasium + PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c46203-30ac-4ff3-81b5-dce392c5d1b7",
   "metadata": {},
   "source": [
    "During development, we will use `CartPole-v1` for faster iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c3a0-ac4d-4cc0-bca4-cf6dcb6e7b8e",
   "metadata": {},
   "source": [
    "### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b350f1c9-d5cf-419b-955b-6cc53ce6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8657b946-8b8d-4d09-b9db-7fb5c8a74fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75dab90-099b-4a0b-a1cc-b36e4e5fc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"CartPole-REINFORCE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2527aa4-3100-4cbe-998c-da029e991a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "gamma = 0.99\n",
    "learning_rate = 1e-3\n",
    "\n",
    "config = {\n",
    "    \"env\": env_name,\n",
    "    \"algo\": \"REINFORCE\",\n",
    "    \"gamma\": gamma,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4ad3c4-9ee1-4ecf-9830-6439a0125cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32),\n",
       " Discrete(2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "video_folder = f\"./videos/{project}_{timestamp}\"\n",
    "video_frequency = 50\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder,\n",
    "    episode_trigger=lambda x: x % video_frequency == 0,\n",
    ")\n",
    "\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204dbce-04e8-420e-9560-019a141dea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfitti\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/journey/wandb/run-20250214_143611-3t6rilti</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti' target=\"_blank\">divine-dove-10</a></strong> to <a href='https://wandb.ai/fitti/CartPole-REINFORCE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/CartPole-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/CartPole-REINFORCE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti' target=\"_blank\">https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7bbd2c341910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a435b-7bb4-4eb0-ac06-56112643cc56",
   "metadata": {},
   "source": [
    "### Defining the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6524037a-6207-4bac-b88f-43ead772ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94f3d9-43cb-44c2-9ccb-29e4dfb65db0",
   "metadata": {},
   "source": [
    "We define both the final policy network for Breakout, as well as the simplified MLP for\n",
    "CartPole, which is probably still overkill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9208ac4-1f70-4d46-ade8-c06fb7ef1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers, for four stacked grayscale 84x84 frames\n",
    "        self.conv1 = nn.Conv2d(in_channels= 4, out_channels=16, kernel_size=5, stride=2) # 16 x 40 x 40\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2) # 32 x 18 x 18\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2) # 32 x 7 x 7\n",
    "        conv_output_size = 32 * 7 * 7\n",
    "\n",
    "        # Fully connected head\n",
    "        self.fc = nn.Linear(conv_output_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4039aef-e51c-475e-8699-9360775dbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_input_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "class PolicyNetworkCartPole(nn.Module):\n",
    "    def __init__(self, n_hiddens=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cartpole_input_size, n_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hiddens, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b23632-3a5c-4626-91cd-171deeafa00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetworkCartPole().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea1a02-1868-4a4a-bb09-74cc41a45795",
   "metadata": {},
   "source": [
    "W&B can watch the network's parameters for us, as well as the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936a987b-c390-47b7-bdb2-5ed480d0cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(policy_net, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5dc49-5c9a-474a-8625-713c7fead232",
   "metadata": {},
   "source": [
    "### Action Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856137e-2c3d-438c-b454-3c787a91f32f",
   "metadata": {},
   "source": [
    "To handle action selection, we turn transform the logits returned from the network\n",
    "into a distribution we can sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b47c179-3f47-4677-9eea-f0e521450e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    logits = policy_net(state)\n",
    "\n",
    "    # Create the distribution and sample from it\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = dist.sample()\n",
    "\n",
    "    # We need the log probability for gradient computation\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d28f7-d7b5-44f0-b4fb-78bcf92b8799",
   "metadata": {},
   "source": [
    "### The REINFORCE Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab6ad9-6885-4ef6-8842-4da5a187aa53",
   "metadata": {},
   "source": [
    "The training loop looks as follows:\n",
    "\n",
    "1. Reset the environment to start a new episode\n",
    "2. At each step:\n",
    "    1. Use the policy network for action selection\n",
    "    2. Step the environment with the chosen action\n",
    "    3. Store the log probability of the action, as well as the reward\n",
    "    4. Continue until the episode is `done`\n",
    "3. Compute the return $G_t$ for each time step $t$\n",
    "4. Compute the policy gradient loss, by summing $-\\sum_t \\log\\pi(a_t|s_t)G_t$\n",
    "5. Zero the gradients\n",
    "6. Perform a backward pass on the loss\n",
    "7. Take a step with the optimizer\n",
    "8. Log reward and loss to W&B\n",
    "9. Repeat until happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038415d7-a4f7-45dd-8aa0-b3be912c420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    gamma: float = 0.99,\n",
    "    num_episodes: int = 10_000,\n",
    "    report_frequency: int = 50,\n",
    "):\n",
    "    try:\n",
    "        for episode in tqdm(range(1, num_episodes + 1), desc=\"Episodes\"):\n",
    "            # Reset the environment\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            # Track log probs and rewards\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "\n",
    "            # Play a full episode\n",
    "            done, truncated = False, False\n",
    "            while not (done or truncated):\n",
    "                # Use the policy network for action selection\n",
    "                action, log_prob = select_action(obs)\n",
    "\n",
    "                # Step the environment with the chosen action\n",
    "                obs, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                # Store the log probability of the action, as\n",
    "                # well as the reward\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "\n",
    "            # Compute the return G_t for each time step t\n",
    "            # and compute the policy gradient loss\n",
    "            T = len(log_probs)\n",
    "            total_reward = 0.0\n",
    "            loss = 0.0\n",
    "\n",
    "            for t in reversed(range(T)):\n",
    "                total_reward = rewards[t] + gamma * total_reward\n",
    "                loss -= log_probs[t] * total_reward\n",
    "\n",
    "            loss /= T\n",
    "\n",
    "            # Zero the gradients, perform backward pass, step optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"return\": total_reward,\n",
    "                    \"loss\": loss,\n",
    "                })\n",
    "\n",
    "            if (\n",
    "                episode == 1\n",
    "                or episode == num_episodes\n",
    "                or episode % report_frequency == 0\n",
    "            ):\n",
    "                print(\n",
    "                    f\"Episode: {episode},\",\n",
    "                    f\"Return: {total_reward:.2f},\",\n",
    "                    f\"Loss: {loss.item():.4f}\",\n",
    "                    end=\"\\t\\t\\r\"\n",
    "                )\n",
    "\n",
    "            if episode % video_frequency == 0:\n",
    "                latest_video = max(\n",
    "                    Path(video_folder).iterdir(),\n",
    "                    key=lambda x: x.stat().st_mtime\n",
    "                )\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"video\": wandb.Video(str(latest_video)),\n",
    "                })\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining stopped manually.\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd0dc5ac-22d9-4b83-9e66-04f3af82e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6801a53d23847c098193fedf7167748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3550, Return: 500.00, Loss: 140.0277\t\t\n",
      "Training stopped manually.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▄▁▄▆▆▄▅▂▄▄▅▂▅▆▆█▆█▇████</td></tr><tr><td>return</td><td>▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▃▂▂▃▅▂▄█▄▃▃▃▄▇▆▇█▄█▆▅█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>139.23518</td></tr><tr><td>return</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-dove-10</strong> at: <a href='https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti' target=\"_blank\">https://wandb.ai/fitti/CartPole-REINFORCE/runs/3t6rilti</a><br> View project at: <a href='https://wandb.ai/fitti/CartPole-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/CartPole-REINFORCE</a><br>Synced 5 W&B file(s), 71 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250214_143611-3t6rilti/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(policy_net, optimizer, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1979b22-eb34-4459-9136-f81db0806328",
   "metadata": {},
   "source": [
    "## Training and Debugging the REINFORCE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b870e5-a3c8-4e70-b8b6-df70d1071e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06dca750-0d63-45b5-8bec-160a3f9acd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"Breakout-REINFORCE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be814d6-eaf4-48ca-b496-34b32d06c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43865bcb-88d6-4b89-8073-b4ab4b14f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"ALE/Breakout-v5\"\n",
    "gamma = 0.99\n",
    "learning_rate = 1e-3\n",
    "\n",
    "config = {\n",
    "    \"env\": env_name,\n",
    "    \"algo\": \"REINFORCE\",\n",
    "    \"gamma\": gamma,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c984f33-ed2d-48de-a08b-8efcb09bbc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Box(0.0, 1.0, (4, 84, 84), float32), Discrete(4))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "video_folder = f\"./videos/{project}_{timestamp}\"\n",
    "video_frequency = 10\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder,\n",
    "    episode_trigger=lambda x: x % video_frequency == 0,\n",
    ")\n",
    "env = gym.wrappers.AtariPreprocessing(\n",
    "    env=env,\n",
    "    frame_skip=4,\n",
    "    scale_obs=True,\n",
    ")\n",
    "env = gym.wrappers.FrameStackObservation(env=env, stack_size=4)\n",
    "\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c1cb0e-e0c3-4f3e-a467-ad2124313891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/journey/wandb/run-20250213_093840-fi4qo55r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r' target=\"_blank\">kind-galaxy-4</a></strong> to <a href='https://wandb.ai/fitti/Breakout-REINFORCE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/Breakout-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/Breakout-REINFORCE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r' target=\"_blank\">https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79859148c980>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d3cef52-526e-4dd9-8069-cd3eb3713c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16871cdd-8ea6-4d9e-bc38-a13dc56e2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f086cb55-5647-4d4a-a10a-3b6d0f6fae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(policy_net, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97549769-b5dc-4a35-afb4-14d595501e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece3403feea6475ab10b28214a985830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000, Return: 0.00, Loss: 0.0000\t\t"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▂▁▆▃▁▁▇▁▂▅▁▇▆██▆▁▁▆▁▁▅▇▆▅▅▆▁▄▅▁▄▁▁▆▅▁▂▁</td></tr><tr><td>return</td><td>▄▃▆▃█▁▂▆▅▁▃▃▅▄▁█▁▅▇▁▄▃▁▁▁▁▄▁▁█▁▁▆▁▆▅▄▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0</td></tr><tr><td>return</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-galaxy-4</strong> at: <a href='https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r' target=\"_blank\">https://wandb.ai/fitti/Breakout-REINFORCE/runs/fi4qo55r</a><br> View project at: <a href='https://wandb.ai/fitti/Breakout-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/Breakout-REINFORCE</a><br>Synced 5 W&B file(s), 1000 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250213_093840-fi4qo55r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(policy_net, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c348d4-edd2-49b2-9f19-21ba5b7b04e2",
   "metadata": {},
   "source": [
    "I can see from training that the loss goes up as the returns go up.\n",
    "It may be useful to switch to the average reward setting, or use some other baseline,\n",
    "so as to avoid exploding gradients.\n",
    "\n",
    "It is also clear that REINFORCE can very easily become stuck in a suboptimal policy, as even after thousands of episodes of Breakout, it is no longer improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda55d2-a154-4e0d-96fe-99a377813668",
   "metadata": {},
   "source": [
    "## Next Steps: Improving and Extending REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96420a45-4e23-4b2d-ad09-dfc7f4043338",
   "metadata": {},
   "source": [
    "1. Using a baseline (advantage estimation):\n",
    "    * This includes the actor-critic methods, which I want to look at next!\n",
    "2. Batch REINFORCE provides more stable updates by updating only after collecting a batch of episodes\n",
    "3. Actor-Critic methods (A2C/A3C) continuously update the policy throughout an episode,\n",
    "   introducing bootstrapping instead of Monte Carlo updates\n",
    "4. PPO is the next step after the basic Actor-Critic methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
