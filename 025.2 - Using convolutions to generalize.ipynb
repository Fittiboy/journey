{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2253c76-3267-448c-9d2a-602e26738f4c",
   "metadata": {},
   "source": [
    "# Day 25 - Using convolutions to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a863c-bae7-40fe-a76b-fee9108eb42c",
   "metadata": {},
   "source": [
    "* I spent a lot of time playing around with convolutional layers in the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f2f1d-1d24-4d8e-831f-a445b108fc28",
   "metadata": {},
   "source": [
    "## Convolutions in action\n",
    "\n",
    "* One interesting thing to look at is the size of the weight tensor of a kernel\n",
    "* In general, the shape is `[n_out, n_in]`, so we can guess that it should be `[16, 3, 3, 3]`\n",
    "* Here, it would represent $C'\\times C\\times H\\times W$, where $C'$ is the number of output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa91964-0a77-4c72-9d90-94aefc70596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 3, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "list(conv.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccf906-3fcc-41a7-8d97-bc5e6dce6d32",
   "metadata": {},
   "source": [
    "## Subclassing `nn.Module`\n",
    "\n",
    "* As a simple, illustrative example, we will reimplement the `Flatten` layer as an `nn.Module` subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db77851-e09d-4719-ad4f-4b12eb7ac40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e42e46-8270-47f0-989f-5bbdcc22d5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3072])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(32, 3, 32, 32)\n",
    "model = Flatten()\n",
    "x = model(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387656b7-d673-4e06-9f28-00872e59cb41",
   "metadata": {},
   "source": [
    "### The `functional` API\n",
    "\n",
    "* The `nn.functional` module provides stateless, true functions\n",
    "* It is usually importes as `F`\n",
    "* Using functions like `F.relu()` or `F.max_pool2d` means that they won't have to be registered as submodules to a Module subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba58255-e202-4fe8-b039-bf53290e8827",
   "metadata": {},
   "source": [
    "## Training our convolution neural network\n",
    "\n",
    "* While we did this in the previous notebook, let us reproduce it here in a slightly altered form to store the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21220391-7b72-49d9-86f7-f0cb5b2d95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5708ff6a-7779-4ac4-9b1c-773e7d6733b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./DLPT/data/\"\n",
    "mean = [0.4913996458053589, 0.48215845227241516, 0.44653093814849854]\n",
    "std = [0.24703224003314972, 0.24348513782024384, 0.26158785820007324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a8bc31-2cd3-4aec-8a62-3c3a701b7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean, std)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, transform=transform)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, transform=transform)\n",
    "cifar10_images = datasets.CIFAR10(data_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c55271-e41c-434d-9a1b-d29dfec7ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"airplane\", \"bird\"]\n",
    "label_map = {0: 0, 2: 1}\n",
    "classes = torch.tensor([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f1391d-e90b-49b6-897d-f4f0662c039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "indices_train = torch.where(torch.isin(torch.tensor(cifar10.targets), classes))[0]\n",
    "indices_val = torch.where(torch.isin(torch.tensor(cifar10_val.targets), classes))[0]\n",
    "\n",
    "samples_train = torch.stack([cifar10[i][0] for i in indices_train])\n",
    "samples_val = torch.stack([cifar10_val[i][0] for i in indices_val])\n",
    "\n",
    "targets_train = torch.tensor([cifar10[i][1] == 2 for i in indices_train]).long()\n",
    "targets_val = torch.tensor([cifar10_val[i][1] == 2 for i in indices_val]).long()\n",
    "\n",
    "set_train = TensorDataset(samples_train, targets_train)\n",
    "set_val = TensorDataset(samples_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ed0b0d-700a-4108-a3c5-30bd7ae4f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 384\n",
    "loader_train = DataLoader(\n",
    "    set_train,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "loader_val = DataLoader(\n",
    "    set_val,\n",
    "    len(set_val),\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fe38e5-f982-4c97-9199-68977b6692fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, data_train, data_val, rep_freq=1000, path=None):\n",
    "    train_losses = np.zeros(n_epochs)\n",
    "    val_losses = np.zeros(n_epochs)\n",
    "    val_accs = np.zeros(n_epochs)\n",
    "\n",
    "    # Exponential weight factor\n",
    "    alpha = 0.3\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(1, n_epochs + 1), desc=\"Epochs\"):\n",
    "        # Initialize batch averages for each epoch\n",
    "        batch_losses_train = []\n",
    "        batch_losses_val = []\n",
    "        batch_accs = []\n",
    "        \n",
    "        # Training phase\n",
    "        for batch_imgs, batch_targets in data_train:\n",
    "            batch_imgs = batch_imgs.to(device=device, non_blocking=True)\n",
    "            batch_targets = batch_targets.to(device=device, non_blocking=True)\n",
    "            batch_out = model(batch_imgs)\n",
    "            loss_train = loss_fn(batch_out, batch_targets)\n",
    "            batch_losses_train.append(loss_train.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            for val_imgs, val_targets in data_val:\n",
    "                val_imgs = val_imgs.to(device=device, non_blocking=True)\n",
    "                val_targets = val_targets.to(device=device, non_blocking=True)\n",
    "                model.eval()\n",
    "                val_out = model(val_imgs)\n",
    "                loss_val = loss_fn(val_out, val_targets)\n",
    "                batch_losses_val.append(loss_val.item())\n",
    "                \n",
    "                preds = val_out.argmax(dim=-1)\n",
    "                targets = val_targets.to(device=device)\n",
    "                total_correct = (targets == preds).sum()\n",
    "                accuracy = total_correct / preds.shape[0]\n",
    "                batch_accs.append(accuracy.item())\n",
    "                model.train()\n",
    "        \n",
    "        # Calculate average metrics for this epoch\n",
    "        epoch_train_loss = np.mean(batch_losses_train)\n",
    "        epoch_val_loss = np.mean(batch_losses_val)\n",
    "        epoch_val_acc = np.mean(batch_accs)\n",
    "        \n",
    "        # Store metrics with exponential weighting\n",
    "        if epoch == 1:\n",
    "            # First epoch: use raw values\n",
    "            train_losses[epoch-1] = epoch_train_loss\n",
    "            val_losses[epoch-1] = epoch_val_loss\n",
    "            val_accs[epoch-1] = epoch_val_acc\n",
    "        else:\n",
    "            # Subsequent epochs: use exponential weighted average\n",
    "            train_losses[epoch-1] = (1 - alpha) * train_losses[epoch-2] + alpha * epoch_train_loss\n",
    "            val_losses[epoch-1] = (1 - alpha) * val_losses[epoch-2] + alpha * epoch_val_loss\n",
    "            val_accs[epoch-1] = (1 - alpha) * val_accs[epoch-2] + alpha * epoch_val_acc\n",
    "\n",
    "        if epoch == 1 or epoch == n_epochs or epoch % rep_freq == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_losses[epoch-1]:.4f},\"\n",
    "                  f\" Validation loss {val_losses[epoch-1]:.4f},\"\n",
    "                  f\" Validation accuracy: {val_accs[epoch-1] * 100:.2f}%\")\n",
    "            if path and epoch_val_acc > best_acc:\n",
    "                print(\"New best accuracy!\")\n",
    "                best_acc = epoch_val_acc\n",
    "                torch.save(model.state_dict(), path)\n",
    "\n",
    "    return train_losses, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27aab00-66b5-4ce2-9976-6e0c55ac687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 32,\n",
    "        kernel_size: int = 3,\n",
    "        padding: int = 1,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = F.dropout2d(out, self.dropout, self.training)\n",
    "        out = F.relu(out)\n",
    "        return F.max_pool2d(out, 2, 2)\n",
    "\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.classifier = nn.Linear(out_features, n_classes)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.flatten(1)\n",
    "        out = self.linear(out)\n",
    "        out = F.dropout(out, self.dropout, self.training)\n",
    "        out = F.relu(out)\n",
    "        return self.classifier(out)\n",
    "\n",
    "\n",
    "class Superman(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvBlock(3, 32, dropout=0.2),\n",
    "            ConvBlock(32, 64, dropout=0.5),\n",
    "            ConvBlock(64, 128, dropout=0.5),\n",
    "        )\n",
    "        self.head = ClassifierHead(2048, 512, n_classes, 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06d5f7d-d719-48e2-a200-c96446811671",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "n_classes = 2\n",
    "model = torch.compile(Superman(n_classes).to(device=device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5101c83-249b-4b6e-9959-de06111c792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd02e206ece40caa95287c5e2edf92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.5943, Validation loss 0.4806, Validation accuracy: 77.80%\n",
      "New best accuracy!\n"
     ]
    }
   ],
   "source": [
    "models_path = \"./DLPT/models/\"\n",
    "model_name = \"superman\"\n",
    "tl, vl, va = training_loop(\n",
    "    n_epochs = 4_000,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    data_train = loader_train,\n",
    "    data_val = loader_val,\n",
    "    rep_freq=100,\n",
    "    path = models_path + model_name + \".pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d276dc-22df-422d-87d4-43ce3f257059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(tl, label=\"Training loss\")\n",
    "ax1.plot(vl, label=\"Validation loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = \"tab:green\"\n",
    "ax2.plot(va * 100, 'g--', label=\"Validation accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title(\"Training Loss, Validation Loss and Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395729f-c673-46e6-9d95-06216ac8cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(Superman(n_classes).to(device=device))\n",
    "model.load_state_dict(torch.load(models_path + model_name + \".pt\",\n",
    "                                 weights_only=True,\n",
    "                                 map_location=device\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514cba3-1a39-47b0-9537-279a32f7de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for samples, targets in loader_val:\n",
    "        preds = model(samples.to(device=device)).argmax(dim=-1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "cifar2_val_imgs = [img\n",
    "                   for img, label in cifar10_images\n",
    "                   if label in [0, 2]]\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "total_correct = (targets == preds).sum()\n",
    "accuracy = total_correct / preds.shape[0]\n",
    "fig.suptitle(f\"Model accuracy: {accuracy * 100:.2f}%\", fontsize=32)\n",
    "\n",
    "for i in range(25):\n",
    "    r = i // 5\n",
    "    c = i % 5\n",
    "    img = cifar2_val_imgs[i]\n",
    "    axes[r, c].imshow(img)\n",
    "    correct = (preds[i] == targets[i]).item()\n",
    "    color = \"green\" if correct else \"red\"\n",
    "    axes[r, c].set_xlabel(class_names[preds[i]], color=color)\n",
    "    \n",
    "    axes[r, c].set_xticks([])\n",
    "    axes[r, c].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d7434-1b95-4eb8-a1cc-f917480441ff",
   "metadata": {},
   "source": [
    "## Model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19661cf-1607-4273-aa41-537112b2af55",
   "metadata": {},
   "source": [
    "### Going deeper to learn more complex structures: Depth\n",
    "\n",
    "* Let us see how we can build a ResNet in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa391d-1f42-499e-8ca4-bbe5adab09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity=\"relu\")\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(x)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba7f34-33a7-4e4e-87c9-e88df7a60839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=100):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[ResBlock(n_chans=n_chans1) for _ in range(n_blocks)])\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dfdd9-9930-47ce-8301-e228cc03012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth=10\n",
    "model = torch.compile(NetResDeep(n_blocks=depth).to(device=device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572db37-b1bc-4291-8545-534ad828df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"net_res_deep_superman\"\n",
    "tl, vl, va = training_loop(\n",
    "    n_epochs = 20_000,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    data_train = loader_train,\n",
    "    data_val = loader_val,\n",
    "    rep_freq=100,\n",
    "    path = models_path + model_name + \".pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c741d5a-1889-4983-a3ae-2cf926cdd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(tl, label=\"Training loss\")\n",
    "ax1.plot(vl, label=\"Validation loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = \"tab:green\"\n",
    "ax2.plot(va * 100, 'g--', label=\"Validation accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title(\"Training Loss, Validation Loss and Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049174f-9986-43f4-8189-ea1df54c4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(NetResDeep(n_blocks=depth).to(device=device))\n",
    "model.load_state_dict(torch.load(models_path + model_name + \".pt\",\n",
    "                                 weights_only=True,\n",
    "                                 map_location=device\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a0b97-9597-464a-9a09-4fb11f031efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for samples, targets in loader_val:\n",
    "        preds = model(samples.to(device=device)).argmax(dim=-1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "cifar2_val_imgs = [img\n",
    "                   for img, label in cifar10_images\n",
    "                   if label in [0, 2]]\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "total_correct = (targets == preds).sum()\n",
    "accuracy = total_correct / preds.shape[0]\n",
    "fig.suptitle(f\"Model accuracy: {accuracy * 100:.2f}%\", fontsize=32)\n",
    "\n",
    "for i in range(25):\n",
    "    r = i // 5\n",
    "    c = i % 5\n",
    "    img = cifar2_val_imgs[i]\n",
    "    axes[r, c].imshow(img)\n",
    "    correct = (preds[i] == targets[i]).item()\n",
    "    color = \"green\" if correct else \"red\"\n",
    "    axes[r, c].set_xlabel(class_names[preds[i]], color=color)\n",
    "    \n",
    "    axes[r, c].set_xticks([])\n",
    "    axes[r, c].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
