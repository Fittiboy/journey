{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0413c8-2a9b-4394-85fa-b1cc1c87382b",
   "metadata": {},
   "source": [
    "# Day 13 - Pretrained Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e7719-87ce-4cd5-86ee-b8dfb331833f",
   "metadata": {},
   "source": [
    "## A pretrained network that recognizes the subject of an image\n",
    "\n",
    "* It is common for researchers to publish code with their papers, as well as weights for a pre-trained network\n",
    "* Here, we use such pre-trained networks to see how they perform on image recognition tasks\n",
    "* To run images through a model, they are transformed into a three-dimensional tensor: Channels, height, and width\n",
    "* The output, here for ILSVRC, is a vector with 1,000 elements, each representing a class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd98efb-d021-455f-8fb8-04c1afb797c1",
   "metadata": {},
   "source": [
    "### Obtaining a pretrained network for image recognition\n",
    "\n",
    "* `torchvision` provides many vision models and tools, as well as  access to pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe44517-0ef5-4d2a-b838-0cc8babb237c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'deeplabv3_mobilenet_v3_large',\n",
       " 'deeplabv3_resnet101',\n",
       " 'deeplabv3_resnet50',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcn_resnet101',\n",
       " 'fcn_resnet50',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'lraspp_mobilenet_v3_large',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'maxvit_t',\n",
       " 'mc3_18',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mvit_v1_b',\n",
       " 'mvit_v2_s',\n",
       " 'quantized_googlenet',\n",
       " 'quantized_inception_v3',\n",
       " 'quantized_mobilenet_v2',\n",
       " 'quantized_mobilenet_v3_large',\n",
       " 'quantized_resnet18',\n",
       " 'quantized_resnet50',\n",
       " 'quantized_resnext101_32x8d',\n",
       " 'quantized_resnext101_64x4d',\n",
       " 'quantized_shufflenet_v2_x0_5',\n",
       " 'quantized_shufflenet_v2_x1_0',\n",
       " 'quantized_shufflenet_v2_x1_5',\n",
       " 'quantized_shufflenet_v2_x2_0',\n",
       " 'r2plus1d_18',\n",
       " 'r3d_18',\n",
       " 'raft_large',\n",
       " 'raft_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 's3d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large',\n",
       " 'swin3d_b',\n",
       " 'swin3d_s',\n",
       " 'swin3d_t',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "models.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbdd4d-771a-42bf-a45b-4c1cb7ac65e3",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "* This architecture won ILSVRC in 2012 by a large margin\n",
    "* The performance of AlexNet was a defining moment in the history of computer vision, leading people to understand how powerful deep learning methods can be for computer vision\n",
    "* By today's standards, AlexNet is now considered small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7407427a-1562-4c7c-bd49-4a6a98b91d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a669509-2a20-4e2c-a9ca-96d5dc45c9a1",
   "metadata": {},
   "source": [
    "* If we had a properly sized input, we could now run `output = alexnet(input)`, as we have fully instantiated an (untrained) model\n",
    "* To make it useful, either we have to train it from scratch, or we simply load pretrained weights\n",
    "* In the `models` module, uppercase names are classes that represent model architectures, while lowercase names are functions that instantiate models with a particular number of layers and units, optionally downloading weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f2867-db28-49b8-bc75-c27e03b2eed1",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "* The `resnet101` function instantiates a 101-layer convolutional neural network\n",
    "* Before the ResNet architecture, in 2015, training such deep models was too difficult, but ResNet made it possible and beat several benchmarks\n",
    "* Happy 10th anniversary, ResNet!\n",
    "* The 101-layer ResNet used here has 44.5 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8258cf53-364c-44b9-a750-bb2a60ffa80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /home/fitti/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
      "100%|███████████████████████████████████████████████████████████████████| 171M/171M [00:01<00:00, 103MB/s]\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
